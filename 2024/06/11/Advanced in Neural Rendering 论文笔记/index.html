<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Advanced in Neural Rendering 论文笔记 - BONESKEEP&#039; BLOG</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="BONESKEEP&#039; BLOG"><meta name="msapplication-TileImage" content="/img/headlogo.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="BONESKEEP&#039; BLOG"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Advanced in Neural Rendering 论文笔记 来源EuroGraphocs 的2022综述论文，2022年3月  ABSTRACT 传统上，场景的合成图像使用 渲染算法 （如光栅化或光线跟踪）生成的； 将特别定义的集合和材质属性表示作为输入。 这些输入定义了实际场景和渲染的内容，称为 场景表示 ，场景由一个或多个目标组成。 示例场景表示是具有伴随纹理的 三角网格（艺术家创建）"><meta property="og:type" content="blog"><meta property="og:title" content="Advanced in Neural Rendering 论文笔记"><meta property="og:url" content="https://boneskeep.github.io/2024/06/11/Advanced%20in%20Neural%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><meta property="og:site_name" content="BONESKEEP&#039; BLOG"><meta property="og:description" content="Advanced in Neural Rendering 论文笔记 来源EuroGraphocs 的2022综述论文，2022年3月  ABSTRACT 传统上，场景的合成图像使用 渲染算法 （如光栅化或光线跟踪）生成的； 将特别定义的集合和材质属性表示作为输入。 这些输入定义了实际场景和渲染的内容，称为 场景表示 ，场景由一个或多个目标组成。 示例场景表示是具有伴随纹理的 三角网格（艺术家创建）"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://boneskeep.github.io/img/og_image.png"><meta property="article:published_time" content="2024-06-11T08:00:00.000Z"><meta property="article:modified_time" content="2024-12-04T07:51:38.299Z"><meta property="article:author" content="BONESKEEP"><meta property="article:tag" content="学术"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://boneskeep.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://boneskeep.github.io/2024/06/11/Advanced%20in%20Neural%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"},"headline":"Advanced in Neural Rendering 论文笔记","image":["https://boneskeep.github.io/img/og_image.png"],"datePublished":"2024-06-11T08:00:00.000Z","dateModified":"2024-12-04T07:51:38.299Z","author":{"@type":"Person","name":"BONESKEEP"},"publisher":{"@type":"Organization","name":"BONESKEEP' BLOG","logo":{"@type":"ImageObject","url":"https://boneskeep.github.io/img/head_circle.png"}},"description":"Advanced in Neural Rendering 论文笔记 来源EuroGraphocs 的2022综述论文，2022年3月  ABSTRACT 传统上，场景的合成图像使用 渲染算法 （如光栅化或光线跟踪）生成的； 将特别定义的集合和材质属性表示作为输入。 这些输入定义了实际场景和渲染的内容，称为 场景表示 ，场景由一个或多个目标组成。 示例场景表示是具有伴随纹理的 三角网格（艺术家创建）"}</script><link rel="canonical" href="https://boneskeep.github.io/2024/06/11/Advanced%20in%20Neural%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><link rel="icon" href="/img/headlogo.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/head_circle.png" alt="BONESKEEP&#039; BLOG" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives/">Archives</a><a class="navbar-item" href="/categories/">Categories</a><a class="navbar-item" href="/tags/">Tags</a><a class="navbar-item" href="/about/">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/BONESKEEP">GitHub</a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/BONESKEEP/BONESKEEP.github.io"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-06-11T08:00:00.000Z" title="2024/6/11 16:00:00">2024-06-11</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:51:38.299Z" title="2024/12/4 15:51:38">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF/">学术</a><span> / </span><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/">三维重建</a></span><span class="level-item">1 小时读完 (大约10041个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">Advanced in Neural Rendering 论文笔记</h1><div class="content"><h3 id="Advanced-in-Neural-Rendering-论文笔记"><a href="#Advanced-in-Neural-Rendering-论文笔记" class="headerlink" title="Advanced in Neural Rendering 论文笔记"></a>Advanced in Neural Rendering 论文笔记</h3><hr>
<p>来源EuroGraphocs 的2022综述论文，2022年3月</p>
<hr>
<p><strong>ABSTRACT</strong></p>
<p>传统上，场景的合成图像使用 <strong>渲染算法</strong> （<strong>如光栅化或光线跟踪</strong>）生成的；</p>
<p>将特别定义的集合和材质属性表示作为输入。</p>
<p>这些输入定义了实际场景和渲染的内容，称为 <strong>场景表示</strong> ，场景由一个或多个目标组成。</p>
<p>示例场景表示是具有伴随纹理的 <strong>三角网格（艺术家创建）</strong>、<strong>点云（来自深度传感器）</strong>、<strong>体网格（来自CT扫描）</strong>或 <strong>隐式曲面函数（截断符号距离场）</strong>；</p>
<p>使用<strong>可微分渲染的损失</strong> 从观测中 重建这样的场景表示 被称为 <strong>逆图形学或逆渲染</strong>。</p>
<p>神经渲染结合了图形学和机器学习的思想，创建了从真实世界观测合成图像的算法；</p>
<p><strong>近年来该领域展示了 可将学习的组件 注入 渲染流水线的不同方法</strong></p>
<p>这篇报告侧重于 将经典渲染原理与学习的3D场景表示（神经场景表示）相结合的方法；</p>
<p>关键优点是在设计上的3D一致性，从而实现了捕获场景的新视点合成等应用；</p>
<p>除了 <strong>处理静态场景的方法</strong>外，还介绍了用于建模 <strong>非刚体变形目标</strong> 的神经场景表示以及场景编辑和合成；</p>
<p>这些方法大多时 <strong>场景特定</strong>的，但也讨论了 <strong>跨目标类进行泛化的技术</strong>，并可用于生成任务；</p>
<p>回顾这些最先进的方法外 还概述了 使用的基本概念和定义，最后讨论了公开挑战和社会影响。</p>
<hr>
<p><strong>1. INTRO</strong></p>
<p>传统计算机图形学可以生成高质量的可控图像，但是要基于物理定律并且场景的所有物理参数，比如 <strong>摄像机参数、照明和对象的材质</strong>都需要作为输入提供；</p>
<p>所以如果想要生成真实场景的可控图像，需要从现有的观测（如图像和视频）中估计这些物理属性，即逆渲染，具有挑战性，尤其是质量达到照片级时的合成图像。</p>
<p><strong>图像合成——需要物理参数——需要进行观测——观测后估计（逆渲染）——达成图像合成目标</strong></p>
<p><strong>挑战性在于逆渲染具有很大的挑战性！</strong></p>
<p>所以，神经渲染出现，允许场景的紧凑（compact）表示，而且可以通过 <strong>神经网络</strong>从现有观测中学习渲染。</p>
<p><strong>神经渲染的主要思想</strong>是：结合经典（基于物理的）计算机图形学的见解和深度学习的最新进展。</p>
<p><strong>神经渲染的目标</strong>：<strong>以可控的方式生成照片级真实感图像</strong>，例如新视点合成、重照明、场景变形和合成等。</p>
<p><strong>早期的神经绘制方法：</strong></p>
<p>使用神经网络将场景参数转换为输出图像，场景参数也可以直接作为<strong>一维输入</strong>给定，或者使用经典的图形学流水线生成<strong>二维输入</strong>。</p>
<p><strong>深度神经网络：</strong></p>
<p>通过对真实世界场景的观察进行训练，并学习对这些场景进行 <strong>建模和渲染</strong>。</p>
<p>具体来说：一个网络根据输入参数、模型架构和可训练参数定义了一个函数族。使用随机梯度下降，从这空间中找到最能解释由训练损失度量的训练集的函数。</p>
<p>神经绘制的目的是寻找控制参数与对应的输出图像之间的映射。</p>
<p>可以理解为一个复杂且具有挑战性的稀疏数据插值问题。因此，类似于经典的函数拟合，<strong>神经绘制需要在欠拟合和过拟合之间进行权衡</strong>。如果网络的表征能力不足，那么得到的图像质量就会很低，<strong>找到合适的网络架构</strong>本身就是一门艺术。在神经渲染的背景下，<strong>设计正确的物理驱动的归纳偏差往往需要强大的图形背景</strong>。</p>
<p>这方面的一个很好的例子是最近的神经渲染技术 <strong>NeRF</strong>，该技术试图<strong>通过仅学习3D场景表示</strong>并<strong>依赖计算机图形中的渲染函数</strong>进行<strong>监督</strong>来<strong>分离建模和渲染过程</strong>。</p>
<p><strong>神经辐射场（NeRF）</strong>使用多层感知器（MLP）来近似3D场景的辐射场和密度场。该学习的体表示可以使用解析可微分渲染（即体积分）从任何虚拟摄像头渲染。</p>
<p>对于训练，假设从多个摄像机视点观测场景。从这些训练视点，渲染估计的3D场景，并最小化渲染图像和观察图像之间的差异，根据这些观察结果训练网络。一旦训练完成，由神经网络近似的3D场景可以从新的视点进行渲染，从而实现可控合成。</p>
<p>与使用神经网络学习渲染函数的方法相反，<strong>NeRF</strong>在该方法中<strong>更明确地使用了计算机图形学的知识</strong>，由于（物理）归纳偏差，能够更好地概括新视图：<strong>场景密度和半径的中间3D结构化表示</strong>。因此，NeRF在3D空间中学习物理上有意义的颜色和密度值，物理激发的光线投射和体集成可以持续渲染到新视图中。</p>
<p>所取得的结果质量，以及方法的简单性，导致了该领域的“爆炸式”发展。已经取得了一些进步，提高了适用性，实现了可控性，动态变化场景的捕获以及训练和推理时间。</p>
<p>神经渲染发展非常快，在许多不同的维度上都取得了重大进展，因此对最近的方法及其应用领域进行了分类，以提供发展的简要概述。</p>
<hr>
<p><strong>2. Scope of This STAR</strong></p>
<p>在本报告中，重点介绍了将经典渲染与可学习3D表示相结合的高级神经渲染方法（见图2）。</p>
<p>先前关于神经渲染的STAR报告主要集中在此范式：<strong>2D神经渲染( 2D Neural Rendering )<strong>，也称为神经细化、神经重渲染或延迟神经渲染，是</strong>基于某个2D信号（例如一个语义标签或者一个栅格化的代理几何）输入</strong> 直接映射到输出图像——神经网络被训练用于渲染，例如使用<strong>经典的渲染器生成并学习在2D中渲染场景</strong>。</p>
<p>本报告聚焦于此范式：<strong>3D神经渲染</strong>学习在3D中表示一个场景，并使用来自计算机图形学的固定可微的渲染方案。例如NeRF，神经网络是有监督的，以表示特定场景的形状或外观，神经表示用略传统的图形“引擎”来呈现，通过分析定义，并不是被学习的。</p>
<p>通过设计，底层的神经3D表示是3D一致的，并且能够控制不同的场景参数。</p>
<p>在本报告中，全面概述了不同的场景表示，并详细介绍了从经典渲染流水线以及机器学习中借鉴的组件基本原理。进一步关注用神经辐射场以及体渲染的方法。然而，这里忽略主要在2D屏幕空间中推理的神经渲染方法，也不包括光线跟踪图像的神经超采样和去噪方法。</p>
<hr>
<p><strong>3. Fundamentals of Neural Rendering</strong></p>
<p>神经渲染管道从真实世界的图像中学习渲染和&#x2F;或表示场景，这些图像可以是无序的图像集，也可以是结构化的多视图图像或视频。</p>
<p>3D神经渲染的一个关键特性是该训练过程中相机捕获过程(即,投影和图像的形成)和3D场景表示的解耦。</p>
<p>这种解耦有许多优点，特别是在图像(例如,对于新颖的视点合成)的合成过程中，可以获得很高的3D一致性。</p>
<p>为了将投影等物理过程从三维场景表示中分离出来，三维神经绘制方法依赖于计算机图形学(例如,栅格化、点散布或体积积分)中已知的图像生成模型。</p>
<p>这些模型受到物理学的启发，特别是发射器的光线与场景以及相机本身的相互作用。这种光传输是使用渲染方程[ Kaj86 ]来描述的。</p>
<p>计算机图形学领域对此渲染方程提供了多种近似。</p>
<p>这些近似依赖于使用的场景表示，范围从经典的栅格化到路径追踪和体积积分。</p>
<p>3D神经渲染利用了这些渲染方法。</p>
<p>下面我们将详细介绍场景表示( 3.1节)以及常用神经绘制方法中使用的渲染方法( 3.2节)。</p>
<p>注意，<strong>为了从真实图像中学习</strong>，<strong>场景表示和渲染方法本身都必须是可微的</strong>( 3.3节)。</p>
<hr>
<p><strong>3.1. 场景表示</strong></p>
<p>几十年来，图形学界探索了各种表征，包括点云、隐式和参数曲面、网格和体积（见图）。</p>
<p>这些表示在 图形学领域有明确定义，但在当前神经渲染的文献中存在混淆，尤其是涉及到 <strong>隐式和显式曲面表示和体积表示</strong>。</p>
<p>通常，体表示可以表示曲面，反之亦然。</p>
<p>体表示存储 <strong>体特征</strong>，比如 <strong>密度、不透明度或占用率</strong>，也可以存储<strong>多维特征</strong>，如<strong>颜色或亮度</strong>。</p>
<p>与体表示不同，曲面表示存储<strong>目标曲面的特性</strong>。</p>
<p><strong>不能</strong>用于<strong>模拟体物质</strong>，如烟雾（除非是粗略近似值）。</p>
<p>对于曲面和体表示，都有<strong>连续和离散</strong>的对应项（见上图）。</p>
<p><strong>连续表示</strong>对于神经渲染方法特别有趣，因为它们<strong>可以提供解析梯度</strong>。</p>
<p>曲面表示（surface representation）有两种不同的方式——显式和隐式</p>
<p>在欧式空间中有显式曲面函数S<del>fexplicit</del>的定义（公式3.1 3.2 3.3）</p>
<p>体积表示（volume representation）定义了整个空间的属性（公式3.4）</p>
<p>以上所有表示，各自的功能领域都可以会被限制。具体见3.1。</p>
<p>在神经渲染的背景下，<strong>使用神经网络来近似（比如基于MLP的函数逼近器）表面或体积表示函数的场景表示</strong>称为<strong>神经场景表示</strong>。特别的，表面和体积表示都可以扩展以存储额外的信息，如颜色或与视角相关的辐射。</p>
<hr>
<p><strong>3.1.1. MLPs as Universal Function Approximators</strong></p>
<p>本节讨论了不同的基于MLP的函数逼近器，其构建了最近的神经曲面和体积表示的基础。</p>
<hr>
<p><strong>3.1.2. Representing Surfaces</strong></p>
<p>本节介绍了不同曲面表示方式，如点云、网格等。</p>
<ul>
<li><strong>点云(Point Clouds)</strong></li>
</ul>
<p><strong>点云</strong>是欧氏空间的元素集合。一个连续的曲面可以通过点云离散化，点云的每个元素代表曲面上的一个样本点( x , y , z)。对于每个点，可以存储额外的属性，如法线或颜色。</p>
<p><strong>具有法线特征</strong>的点云也被称为<strong>定向点云</strong>。除了简单的点可以看成无穷小的表面面片外，还可以使用半径为(表示位于下垫面切平面上的2D圆盘)的定向点云。这种表象被称为表面元素，别名面元[ PZvBG00 ]。</p>
<p>它们经常被用于计算机图形学中，用于从模拟中渲染点云或粒子。这样的面元的渲染被称为抛雪球，最近的工作表明它是<strong>可微的</strong>。</p>
<p>使用这种可微的渲染管线，可以直接反向传播到点云位置以及伴随特征(例如,半径或颜色)。</p>
<p>在Neural基于点的图形学和SynSin中，可学习的特征<strong>被附加到</strong>能够<strong>存储</strong>关于<strong>实际表面的外观和形状</strong>的丰富<strong>信息</strong>的<strong>点</strong>上。</p>
<p>在ADOP中，这些可学习的特征由一个可以解释视图依赖效应的MLP来解释。值得注意的是，对于<strong>离散</strong>位置，也可以使用MLP来预测特征，而不是存储特定点的显式特征。</p>
<p>如前所述，点云是欧氏空间的元素集合，因此，除了曲面，它们<strong>还可以表示体</strong>(例如,存储额外的不透明度或密度值)。对每个点使用一个半径自然会导致一个完整的基于球的公式。</p>
<ul>
<li><strong>网格(Meshes)</strong></li>
</ul>
<p><strong>网格</strong>。多边形网格表示曲面的分段线性逼近。特别地，三角网格和四边形网格在计算机图形学中被用作表面事实上的标准表示。</p>
<p>优化了图形流水线和图形加速器( GPU )，以每秒处理和光栅化数十亿个三角形。</p>
<p>大多数图形编辑工具使用<strong>三角形网格</strong>，这使得这种表示对于任何内容创建管道都很重要。为了与这些管线直接兼容，许多”经典”的逆向图形和神经绘制方法都使用了这种基本的曲面表示。</p>
<p>使用<strong>可微</strong>的渲染器，可以<strong>优化顶点位置和顶点属性</strong>(例如,颜色)，以再现一幅图像。</p>
<p>神经网络可以被训练来<strong>预测顶点的位置</strong>，例如，预测动态变化的曲面。而不是使用顶点属性，在三角形内存储表面属性的常用策略是<strong>纹理贴图</strong>。</p>
<p>2D纹理坐标是贴在网格顶点上的，它参考了纹理图像中的一个位置。利用重心插值，可以计算三角形内任意一点的纹理坐标，并利用双线性插值从纹理中检索属性。纹理的概念也被集成到了标准的图形流水线中，增加了一些额外的功能，如分级细化，它需要妥善处理纹理( c.f. ,采样定理)的采样。</p>
<p><strong>延迟神经绘制</strong>( Deferred Neural Rendering )使用包含可学习的视点相关特征的纹理，即所谓的<strong>神经纹理</strong>。具体来说，一个粗网格被用作底层的3D表示，以光栅化这些神经纹理。神经网络在图像空间中解释这些光栅化的特征。注意到该网络可以是一个像素级的MLP，那么神经纹理就代表了表面辐亮度。</p>
<p>相对于使用离散纹理，可以使用连续纹理。纹理场的作者提出使用MLP来预测每个表面点的颜色值。在<strong>神经反射场纹理( NeRF-Tex )</strong> 中，将NeRF的思想与使用2D神经纹理和底层3D网格的思想相结合。NeRF - Tex是<strong>基于用户定义的参数来控制外观</strong>，因此<strong>可以被</strong>艺术家<strong>编辑</strong>。</p>
<ul>
<li><strong>隐式曲面(Implicit Surfaces)</strong></li>
</ul>
<p><strong>隐式曲面</strong>。隐式曲面将曲面定义为函数的零水平集，见公式3 。最常用的隐式曲面表示方法是符号距离函数( SDF )。这些SDF表示被用于许多使用体积融合增量重建静态或动态物体表面的3D扫描技术中。<strong>隐式曲面表示</strong>提供了许多优点，因为它们<strong>避免了定义网格模板的要求</strong>，因此<strong>能够在动态场景中表示拓扑未知或拓扑变化的物体</strong>。</p>
<p>上述体积融合方法使用<strong>离散</strong>(截断)的符号距离函数，即使用<strong>包含符号距离值</strong>的3D网格。Hoppe等人提出了分段线性函数来对输入曲面点样本的符号距离函数进行建模。Carr等人的开创性工作使用径向基函数网络代替。这个径向基函数网络代表了一个连续的隐式曲面函数，可以看作是第一个”神经”隐式曲面表示。</p>
<p><strong>最近的</strong>神经隐式曲面表示是<strong>基于坐标的多层感知器</strong>( MLPs )，见3.1.1节。这种表示方法在<strong>神经场景的表示和渲染中</strong>得到了<strong>广泛应用</strong>。它们在[ PFS⋅19 , CZ19]中同时被提出<strong>用于形状建模</strong>，其中MLP架构用于将连续坐标映射到符号距离值。这种坐标网络表示的信号的保真度，或者说神经隐式表示，<strong>主要受限于网络的容量</strong>。因此，与上述其他表示法相比，隐式曲面<strong>在记忆效率方面具有潜在的优势</strong>，并且作为一种<strong>连续表示法</strong>，<strong>理论上</strong>可以<strong>表示无限分辨率的几何图形</strong>。</p>
<p>最初的建议接踵而至，产生了广泛的热情，随后有了各种针对不同侧重点的改进，包括<strong>训练方案、利用全局-局部上下文、采用特定参数化或空间分区</strong>。<strong>（创新点）</strong></p>
<p>由于<strong>不需要预先定义网格模板或对象拓扑结构</strong>，<strong>神经隐式曲面</strong>非常<strong>适合对不同拓扑结构的对象</strong>进行建模。</p>
<p><strong>利用反向传播</strong>可以<strong>计算输出相对于输入坐标的解析梯度</strong>。这使得除了其他几何激励的正则化器外，在梯度上实现正则化项成为可能。</p>
<p>这些表示可以扩展到对场景的辐亮度进行编码。这对于神经渲染是有用的，在这里我们<strong>希望</strong>场景表示能够同时<strong>编码场景的几何和外观</strong>。</p>
<hr>
<p><strong>3.1.3. Representing Volumes</strong></p>
<p>本节介绍了不同体积表示方式，如体素网格、神经体积表示等。</p>
<ul>
<li><strong>体素网格( Voxel Grids )</strong></li>
</ul>
<p>体素网格( Voxel Grids )。作为<strong>R^3^</strong>中的像素等价体，体素通常被用来<strong>表示体积</strong>。</p>
<p>它们可以<strong>存储几何占有率</strong>，或者存储<strong>具有体积效应</strong>(如<strong>透明度</strong>)的场景的密度值。</p>
<p>此外，场景的外观可以存储。</p>
<p>使用三线性插值，这些体属性可以在体素网格内的任意点访问。这种插值方法特别适用于光线投射等基于样本的绘制方法。</p>
<p>在存储的属性可以具有特定的语义(例如,占有率)的同时，属性也可以被学习。</p>
<p>西茨曼等人提出使用<strong>DeepVoxels</strong>，将<strong>特征</strong>存储在体素网格中。光线<strong>投射渲染</strong>程序后的<strong>特征的积累和解释</strong>是使用深度<strong>神经网络</strong>完成的。这些DeepVoxels可以看作是<strong>体积神经纹理</strong>，可以直接使用<strong>反向传播</strong>进行优化。</p>
<p>虽然基于稠密体素的表示可以快速查询，但它们的内存效率很低，并且3D CNNs可能在这些体上运行，计算量很大。</p>
<p><strong>八叉树</strong>数据结构（用于<strong>描述空间</strong>的树状数据结构）可以用稀疏的方式来表示体积。八叉树上的稀疏3D卷积可以帮助缓解一些问题，但这些紧凑的数据结构不能轻易地在工作中更新。因此，它们很难融入到学习框架中。</p>
<p>其他<strong>减轻密集体素网格内存</strong>挑战的方法包括使用对象特定形状模板，多平面或多球体图像，它们都旨在使用<strong>稀疏近似</strong>来表示体素网格。</p>
<ul>
<li><strong>神经体积表征(Neural Volumetric Representations)</strong></li>
</ul>
<p>神经体积表征(Neural Volumetric Representations)。特征或其他感兴趣的量也可以<strong>使用神经网络</strong>来定义，类似于<strong>神经隐式曲面</strong>(见3.1.2节)。MLP网络体系结构可以用于参数化体积，与显式体素网格相比，MLP网络体系结构具有更高的存储效率。</p>
<p>这些表示可能仍然成本巨大，需要<strong>根据基础网络的大小进行采样</strong>，因为对于每个样本，必须计算通过网络的整个前馈。大多数方法可以粗略地分类为使用全局或局部网络。</p>
<p>同时使用网格和神经网络的混合表示在计算效率和内存效率之间做出了权衡。</p>
<p>与神经隐式曲面类似，神经体积表示<strong>允许计算解析梯度</strong>，这<strong>已被用于定义正则化项</strong>。</p>
<p>BACON引入了<strong>基于带限坐标</strong>的网络，学习了<strong>表面的光滑多尺度分解</strong>。</p>
<ul>
<li><strong>总的注释(General remark)</strong></li>
</ul>
<p>总的注释(General remark)：</p>
<p>使用<strong>基于坐标</strong>的神经网络对场景<strong>进行体积建模</strong>(如<strong>NeRF</strong>)，表面上类似于使用坐标网络对表面进行隐式建模(如在神经隐式曲面中)。</p>
<p>然而，类NeRF的<strong>体表示并不一定是隐式</strong>的——因为网络的输出是<strong>密度和颜色</strong>，场景的几何结构是由<strong>网络显式</strong>地，而不是隐式地参数化的。</p>
<p>尽管如此，这些模型在文献中仍然被称为”隐式”是很常见的，这可能是由于场景的<strong>几何形状</strong>是由<strong>神经网络</strong>（SDF文献中使用了一个不同的”隐式”定义）的**权重”隐式”**定义的。</p>
<p>同时注意到，这是一个与深度学习和统计社区通常使用的”隐式”不同的定义，其中**”隐式”<strong>通常是</strong>指模型的输出被隐式地定义为动态系统的固定点<strong>，其</strong>梯度使用隐函数定理计算**。</p>
<hr>
<p><strong>3.2. 可微图像的生成 Differentiable Image Formation</strong></p>
<p>前面几节中的场景表示允许我们表示场景的3D几何和外观。</p>
<p>下一步，我们将描述<strong>如何通过渲染从这些场景表示中生成图像</strong>。</p>
<p>将三维场景渲染成二维图像平面的一般方法有两种：光线投射和栅格化，见图4。</p>
<p>也可以通过<strong>定义场景中的相机</strong>来<strong>计算场景的渲染图像</strong>。</p>
<p>大多数方法使用<strong>针孔相机</strong>，其中所有相机**光线都通过空间中的单点(焦点)**。</p>
<p>对于<strong>给定的相机</strong>，可以<strong>将来自相机原点的光线投射到场景中</strong>，以便<strong>计算渲染图像</strong>。</p>
<p>(a) <strong>正向渲染</strong>(例如,光栅化)——通过 <strong>将三维表示投影到图像平面上</strong> 生成图像。</p>
<p>(b) <strong>光线投射</strong>( Ray Casting )——通过<strong>投射观看光线</strong>，<strong>采样3D表示</strong>并<strong>累加生成图像</strong>。</p>
<p><strong>上图描述</strong>：</p>
<p>对于<strong>显式曲面表示</strong>，曲面是直接可索引的。这使得我们可以使用前向渲染方法，将表面投影到图像平面，并相应地设置一个像素(例如,使用栅格化或点散布)。<strong>隐式表面表示和体积表示</strong>，<strong>不提供</strong>允许前向渲染的表面的<strong>直接信息</strong>，相反，从虚拟相机看到的3D空间必须采样以生成图像(例如,使用<strong>光线行进</strong>法)。</p>
<p><strong>光线投射</strong>。在针孔模型中，<strong>基本截距定理</strong>可以用来描述<strong>一个点p∈R^3^在三维中如何被投影到图像平面中的正确位置q∈R^2^</strong>。</p>
<p>它被定义为一个非内射函数，并且很难求逆，这使得它在三维重建问题中处于核心地位。</p>
<p><strong>Pinhole模型</strong>对该投影具有单一参数矩阵：<strong>本征矩阵K</strong>包含经像素尺寸归一化的<strong>焦距f &#x3D; [α<del>x</del> , α<del>y</del>]<strong>，</strong>轴偏斜度γ</strong>和<strong>中心点c</strong> &#x3D; [c<del>x</del> , c<del>y</del>]。利用截距定理并假设齐次坐标p′&#x3D; [ x , y , z , 1]，我们发现投影坐标为q′&#x3D; K · p′，有</p>
<p>这假设投影的中心在坐标原点，并且相机是轴对齐的。为了<strong>推广到任意相机位置</strong>，可以使用一个<strong>外部矩阵R</strong>。这个均匀的4 × 4矩阵E是由</p>
<p>其中<strong>R是旋转矩阵</strong>，t是平移向量，使得<strong>R · pw + t &#x3D; pc</strong>，这里我们用<strong>pw表示世界坐标中的点</strong>，pc表示相机坐标中的点。R和t的这种定义在计算机视觉(例如, OpenCV使用的)中很常见，被称为”世界到世界”映射，而在计算机图形学(例如,在OpenGL中)中，类似的逆”凸轮到世界”映射更普遍。假设” worldto-cam “约定，利用齐次坐标，我们可以写出<strong>pw到qp的全投影</strong>为：</p>
<p>如果使用’ cam-to-world ‘约定，光线投射也同样方便。由于深度模糊，这些方程是非单射的，但它们非常<strong>适合于自动微分</strong>，并且可以在<strong>图像形成模型</strong>中进行端到端的优化。</p>
<p>为了正确地对当前的相机进行建模，还必须考虑另外一个部件：<strong>镜头</strong>。</p>
<p>抛开在图像形成过程中必须建模的景深或运动模糊等影响，它们<strong>会给投影函数添加失真影响</strong>。不幸的是，没有一个单一的、简单的模型来捕获所有不同的镜头效果。</p>
<p>标定包，如OpenCV中的标定包，通常实现最多12个畸变参数的模型。它们是通过五次多项式建模的，因此不是平凡可逆的(这是光线投射相对于点投影所需要的)。</p>
<p><strong>更现代的摄像机</strong>标定方法使用了更多的参数，并达到了更高的精度，并且可以实现可逆和可微。</p>
<ul>
<li><strong>光栅化&#x2F;栅格化 Rasterization</strong></li>
</ul>
<p>光栅化。光线投射的一种替代方法是将几何图元栅格化。</p>
<p>该技术并不试图模仿真实世界的图像形成过程，而是利用物体的几何特性来快速生成图像。</p>
<p>它主要用于网格，由顶点v和面f的集合描述，连接顶点的三元组或四元组来定义曲面。</p>
<p>一个基本的见解是，3D中的几何操作可以单独处理顶点：例如，我们可以使用相同的外部矩阵E将世界中的每个点变换到相机坐标系中。</p>
<p>经过这种变换后，<strong>可以剔除视锥体之外的点或法线方向错误的点</strong>，以减少下一步需要处理的点和面的数量。</p>
<p>通过使用如上所述的内蕴矩阵K，可以再次平凡地找到投影到图像坐标的其余点的位置。</p>
<p>利用人脸信息可以对人脸图元进行深度插值，最上层的人脸可以存储在一个零缓冲器中。</p>
<p>这种实现投影的方式<strong>往往比光线投射更快</strong>：它主要根据场景中<strong>可见顶点的数量</strong>进行缩放，而光线投射则根据<strong>像素的数量和要与之相交的图元的数量</strong>进行缩放。然而，使用(例如,光影、阴影、反射等)来捕获某些效应是比较困难的。它可以通过”软”栅格化使其具有<strong>可微性</strong>。例如，在[ LLCL19 , RRN⋅20]中已经实现。</p>
<hr>
<p><strong>3.2.1. 曲面渲染 Surface Rendering</strong></p>
<ul>
<li><strong>点云渲染 Point Cloud Rendering</strong></li>
</ul>
<p>点云渲染。点云是<strong>连续曲面或体积</strong>的<strong>离散样本</strong>。</p>
<p>点云渲染对应于从不规则分布的离散样本中重建连续信号，例如连续表面的出现，然后在每个像素位置的图像空间中重采样重建信号。</p>
<p>这个过程可以通过<strong>两种不同的方式</strong>来完成：</p>
<p>第一种方法<strong>基于经典信号处理理论</strong>，可以看作是一个”软”点抛雪球（Splatting 喷溅）(类似于下面网格渲染部分中的软光栅化器)。它首先利用连续的局部重构核r ( · )构造连续信号，即f &#x3D;∑f i ( pi )。</p>
<h6 id="本质上，这种方法相当于将离散样本与一些局部确定性模糊核混合，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。"><a href="#本质上，这种方法相当于将离散样本与一些局部确定性模糊核混合，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。" class="headerlink" title="本质上，这种方法相当于将离散样本与一些局部确定性模糊核混合，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。"></a>本质上，这种方法相当于<strong>将离散样本与一些局部确定性模糊核混合</strong>，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。</h6><p>相应地，前述步骤有效地将个体特征投影并融合到2D特征图中。</p>
<p>如果特征具有预定义的语义(例如,颜色,法线)，可以使用固定的着色函数或BRDF来生成最终的图像。</p>
<p>如果特征是学习到的神经描述符，则部署2D神经网络将2D特征图转换为RGB图像。</p>
<h6 id="最近采用这种方法的神经点绘制方法包括Sin-Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。"><a href="#最近采用这种方法的神经点绘制方法包括Sin-Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。" class="headerlink" title="最近采用这种方法的神经点绘制方法包括Sin Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。"></a>最近采用这种方法的神经点绘制方法包括Sin Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。</h6><p>虽然这些<strong>简化的内核会导致渲染伪影</strong>，如空洞，模糊边缘和锯齿，但这些<strong>伪影可以在神经着色</strong>步骤中<strong>得到补偿</strong>，并且在Pulsar的情况下，可以通过优化半径来实现。[ KPLD21 ]还使用了相机选择和概率深度测试的策略，并能够在该框架中处理IBR、风格化和协调。</p>
<p>除了<strong>软点绘制方法</strong>外，还可以使用传统的OpenGL或directx技术的<strong>点渲染器</strong>。在这里，每个点被投影到单个像素(或小面积的像素)，从而得到一个稀疏的特征图。可以使用深度神经网络直接在图像空间中重构信号。注意，这种朴素的渲染方法不提供关于点位置p的梯度，只允许区分渲染函数w . r . t . (神经)特征。相比之下，软点散布方法通过重构核r ( p )提供点位置梯度。</p>
<h6 id="然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。-YSW※19b-通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献-RFS21b-中被采用，以联合优化几何和相机姿态来进行新的视图合成。"><a href="#然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。-YSW※19b-通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献-RFS21b-中被采用，以联合优化几何和相机姿态来进行新的视图合成。" class="headerlink" title="然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。[ YSW※19b ]通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献[ RFS21b ]中被采用，以联合优化几何和相机姿态来进行新的视图合成。"></a>然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。[ YSW※19b ]通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献[ RFS21b ]中被采用，以联合优化几何和相机姿态来进行新的视图合成。</h6><ul>
<li><strong>网格渲染 Mesh Rendering</strong></li>
</ul>
<p>有许多通用的渲染器<strong>允许网格被栅格化或以可微的方式渲染</strong>。在可微网格栅格化中，Loper和Black开发了一个可微的渲染框架，称为OpenDR，它<strong>近似一个主要的渲染器</strong>，并<strong>通过自动微分来计算梯度</strong>。</p>
<p><strong>神经网格渲染器( NMR )</strong> 使用可见度变化的手工函数来近似光栅化操作的向后梯度。</p>
<h6 id="文献-LTJ18-提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人-PBDCO19-提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，-LLCL19-提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。"><a href="#文献-LTJ18-提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人-PBDCO19-提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，-LLCL19-提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。" class="headerlink" title="文献[ LTJ18 ]提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人[ PBDCO19 ]提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，[ LLCL19 ]提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。"></a>文献[ LTJ18 ]提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人[ PBDCO19 ]提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，[ LLCL19 ]提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。</h6><h6 id="虽然大多数栅格化器只支持基于直接光照的渲染，但-LHL⋅21-也支持软阴影的可微渲染。在基于物理的绘制领域，-LADL18a-和-ALKN19-引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba-2-NDVZJ19-和Taichi-HLA-19-HAL-20-是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。"><a href="#虽然大多数栅格化器只支持基于直接光照的渲染，但-LHL⋅21-也支持软阴影的可微渲染。在基于物理的绘制领域，-LADL18a-和-ALKN19-引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba-2-NDVZJ19-和Taichi-HLA-19-HAL-20-是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。" class="headerlink" title="虽然大多数栅格化器只支持基于直接光照的渲染，但[ LHL⋅21 ]也支持软阴影的可微渲染。在基于物理的绘制领域，[ LADL18a ]和[ ALKN19 ]引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba 2 [ NDVZJ19 ]和Taichi [ HLA * 19 , HAL * 20]是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。"></a>虽然<strong>大多数栅格化器只支持基于直接光照的渲染</strong>，但[ LHL⋅21 ]也支持软阴影的可微渲染。在基于物理的绘制领域，[ LADL18a ]和[ ALKN19 ]引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba 2 [ NDVZJ19 ]和Taichi [ HLA * 19 , HAL * 20]是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。</h6><ul>
<li><strong>神经隐式曲面渲染 Neural Implicit Surface Rendering</strong></li>
</ul>
<p>当<strong>输入观测为2D图像</strong>时，实现隐式曲面的网络不仅<strong>产生几何相关的量</strong>，即符号距离值，而且<strong>产生外观相关的量</strong>。</p>
<p>一个隐式可微渲染器可以通过首先使用神经隐式函数的几何分支找到一条观察光线与曲面的交点，然后从外观分支获得该点的RGB值来实现。曲面求交的搜索通常是基于球面追踪算法[ Har96 ]的一些变体。</p>
<p>球面跟踪在视线方向上从相机中心对三维空间进行迭代采样，直到到达表面。<strong>球面追踪是一种优化的射线追踪方法</strong>，通过前一位置采样的SDF值来调整步长，但这种迭代策略仍然会带来计算上的开销。泷川等人[ TLY※21 ]通过将<strong>光线跟踪算法与稀疏八叉树数据结构相适应</strong>，<strong>提高了渲染性能</strong>。</p>
<p>在二维监督的联合几何和外观估计的隐式曲面渲染中，一个常见的问题是几何和外观的模糊性。在[ NMOG20 , YKM * 20 , KJJ * 21 , BKW21]中，<strong>从2D图像中提取前景掩码</strong>，为几何分支<strong>提供额外的监督信号</strong>。最近，[ OPG21 ]和[ YGKL21b ]通过<strong>将表面函数转化为体渲染公式</strong>(下面介绍)来解决这个问题；另一方面[ ZYQ21 ]利用现成的深度估计方法生成伪地面真值符号距离值来辅助几何分支的训练。</p>
<hr>
<p><strong>3.2.2 体积渲染 Volumetric Rendering</strong></p>
<p>体积渲染是<strong>基于光线投射</strong>的，并且已经被证明在神经渲染，特别是在从<strong>多视图输入数据</strong>中<strong>学习</strong>场景表示方面是有效的。具体来说，这个场景表示为<strong>体积密度或占有率</strong>的连续场，而不是硬表面的集合。</p>
<p>这意味着<strong>光线</strong>在<strong>空间中的每一点</strong>都有一定的概率<strong>与场景内容发生交互</strong>，而不是二进制的相交事件。</p>
<p>这种连续模型可以很好地作为机器学习管道的可微渲染框架，这些机器学习管道<strong>严重依赖良好行为梯度的存在</strong>进行优化。</p>
<p>虽然完全通用的体积渲染确实考虑了光线可以从一个体粒子反射出去的”散射”事件，但我们将这一总结限制在用于<strong>视图合成</strong>的神经体积渲染方法常用的基本模型中，它只考虑了光线被<strong>一个体粒子</strong>发射或阻挡的”发射”和”吸收”事件。</p>
<p><strong>给定</strong>一组像素<strong>坐标</strong>，我们可以使用先前描述的相机模型<strong>计算通过原点p和方向ω<del>o</del>的3D空间的相应光线</strong>。沿着这条射线的入射光可以用一个简单的发射&#x2F;吸收模型定义为</p>
<p>其中σ为点的体积密度，L<del>e</del>为点和方向的出射光，透射率T为嵌套积分表达式</p>
<p><strong>密度</strong>表示<strong>光线在特定点与场景的体积”介质”相互作用的微分概率</strong>，而<strong>透射率</strong>描述了<strong>光线从点p + tω<del>o</del>回到相机时被衰减的程度</strong>。</p>
<p>这些表达式只能针对简单的密度场和色场进行解析计算。在实际应用中，我们通常<strong>用求积来近似积分</strong>，其中σ和Le被假设为在N个区间{ [ ti - 1，ti ) } ^N^<del>i&#x3D;1</del>内的分段常数，该区间划分了射线的长度：</p>
<p>对于这个近似的完整推导，我们参考Max和Chen 。注意，当写成这种形式时，逼近L的表达式恰好对应于<strong>由后向前合成颜色L^(i)^<del>e</del>的alpha</strong>。</p>
<p><strong>NeRF和相关方法</strong>使用可微体积渲染<strong>将场景表示投影到2D图像中</strong>。这使得这些方法可以<strong>在”逆向渲染”框架中使用</strong>，其中三维或更高维的场景表示是从2D图像中估计出来的。体绘制需要沿一条射线处理多个样本，每个样本都需要通过网络进行完整的前向传递。</p>
<p>最近的工作<strong>提出了增强数据结构、重要性采样、快速集成</strong>等策略来<strong>加快渲染速度</strong>，尽管这些方法的<strong>训练时间仍然较慢</strong>。<strong>自适应坐标网络</strong>使用多分辨率网络结构加速训练，在训练阶段通过以最优和有效的方式分配可用的网络容量来优化网络结构。</p>
<hr>
<p><strong>3.3. 优化 Optimization</strong></p>
<p><strong>训练</strong>神经网络的<strong>核心</strong>是非线性优化，其目的是<strong>应用训练集的约束</strong>，以<strong>获得一组神经网络权值</strong>。</p>
<p>因此，由神经网络逼近的函数能够拟合给定的训练数据。通常，神经网络的优化是基于梯度的；更具体地说，使用了SGD的变体，如Momentum或Adam，其中梯度是通过反向传播算法获得的。</p>
<p>在神经渲染的背景下，神经网络实现了3D场景表示，训练数据由场景的2D观测值组成。使用神经场景表示的可微渲染得到的渲染结果与使用给定的观察进行了比较。</p>
<p>这些重建损失可以用每个像素的L1或L2项来实现，也可以使用基于感知的甚至基于判别器的损失公式。然而，关键是这些损失直接与各自的可微渲染公式耦合，以更新场景表示，见3.1节。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Advanced in Neural Rendering 论文笔记</p><p><a href="https://boneskeep.github.io/2024/06/11/Advanced in Neural Rendering 论文笔记/">https://boneskeep.github.io/2024/06/11/Advanced in Neural Rendering 论文笔记/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>BONESKEEP</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2024-06-11</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2024-12-04</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E5%AD%A6%E6%9C%AF/">学术</a></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechat.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2024/06/11/3D%20Gaussian%20Splatting%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">3D Gaussian Splatting 论文笔记</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/06/11/NeRF%E5%85%A5%E9%97%A8%E5%8F%8A%E8%BF%9B%E9%98%B6%E2%80%94%E2%80%94%E4%B8%AD%E6%81%A9%E5%AE%9E%E9%AA%8C%E5%AE%A4/"><span class="level-item">NeRF入门及进阶——中恩实验室</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/head_circle.png" alt="BONESKEEP"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">BONESKEEP</p><p class="is-size-6 is-block">硕士研究生</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Lanzhou University of Technology</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">46</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">6</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/BONESKEEP" target="_blank" rel="me noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/BONESKEEP"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="QQ" href="https://qm.qq.com/q/4kKCYBUmaA"><i class="fab fab fa-qq"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Mail" href="mailto:coolhui2020@163.com"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Advanced-in-Neural-Rendering-论文笔记"><span class="level-left"><span class="level-item">1</span><span class="level-item">Advanced in Neural Rendering 论文笔记</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#本质上，这种方法相当于将离散样本与一些局部确定性模糊核混合，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">本质上，这种方法相当于将离散样本与一些局部确定性模糊核混合，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。</span></span></a></li><li><a class="level is-mobile" href="#最近采用这种方法的神经点绘制方法包括Sin-Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">最近采用这种方法的神经点绘制方法包括Sin Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。</span></span></a></li><li><a class="level is-mobile" href="#然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。-YSW※19b-通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献-RFS21b-中被采用，以联合优化几何和相机姿态来进行新的视图合成。"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。[ YSW※19b ]通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献[ RFS21b ]中被采用，以联合优化几何和相机姿态来进行新的视图合成。</span></span></a></li><li><a class="level is-mobile" href="#文献-LTJ18-提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人-PBDCO19-提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，-LLCL19-提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">文献[ LTJ18 ]提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人[ PBDCO19 ]提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，[ LLCL19 ]提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。</span></span></a></li><li><a class="level is-mobile" href="#虽然大多数栅格化器只支持基于直接光照的渲染，但-LHL⋅21-也支持软阴影的可微渲染。在基于物理的绘制领域，-LADL18a-和-ALKN19-引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba-2-NDVZJ19-和Taichi-HLA-19-HAL-20-是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。"><span class="level-left"><span class="level-item">1.5</span><span class="level-item">虽然大多数栅格化器只支持基于直接光照的渲染，但[ LHL⋅21 ]也支持软阴影的可微渲染。在基于物理的绘制领域，[ LADL18a ]和[ ALKN19 ]引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba 2 [ NDVZJ19 ]和Taichi [ HLA * 19 , HAL * 20]是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/BONESKEEP" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="https://boneskeep.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Blog</span></span><span class="level-right"><span class="level-item tag">boneskeep.github.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Game/"><span class="level-start"><span class="level-item">Game</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Game/Games101/"><span class="level-start"><span class="level-item">Games101</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Kunpeng/"><span class="level-start"><span class="level-item">Kunpeng</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Kunpeng/Limb/"><span class="level-start"><span class="level-item">Limb</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">28</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/App/"><span class="level-start"><span class="level-item">App</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/"><span class="level-start"><span class="level-item">学术</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/3DGS/"><span class="level-start"><span class="level-item">3DGS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/NeRF/"><span class="level-start"><span class="level-item">NeRF</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"><span class="level-start"><span class="level-item">三维重建</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/%E5%91%BD%E4%BB%A4/"><span class="level-start"><span class="level-item">命令</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/Leetcode/"><span class="level-start"><span class="level-item">Leetcode</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2024/12/04/2024-12-04-Leetcode-Hot100-No.121-%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BA-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"><img src="/img/leetcode/Shangri-La%20Frontier-01.jpg" alt="Leetcode Hot100 No.121 买卖股票的最佳时机 动态规划"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-12-04T14:52:00.000Z">2024-12-04</time></p><p class="title"><a href="/2024/12/04/2024-12-04-Leetcode-Hot100-No.121-%E4%B9%B0%E5%8D%96%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E6%97%B6%E6%9C%BA-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/">Leetcode Hot100 No.121 买卖股票的最佳时机 动态规划</a></p><p class="categories"><a href="/categories/%E7%AE%97%E6%B3%95/">算法</a> / <a href="/categories/%E7%AE%97%E6%B3%95/Leetcode/">Leetcode</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/12/02/2024-12-04-Leetcode-Hot100-No.11-%E7%9B%9B%E6%9C%80%E5%A4%9A%E6%B0%B4%E7%9A%84%E5%AE%B9%E5%99%A8/"><img src="/img/leetcode/Shangri-La%20Frontier-01.jpg" alt="Leetcode Hot100 No.11 盛最多水的容器"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-12-02T14:52:00.000Z">2024-12-02</time></p><p class="title"><a href="/2024/12/02/2024-12-04-Leetcode-Hot100-No.11-%E7%9B%9B%E6%9C%80%E5%A4%9A%E6%B0%B4%E7%9A%84%E5%AE%B9%E5%99%A8/">Leetcode Hot100 No.11 盛最多水的容器</a></p><p class="categories"><a href="/categories/%E7%AE%97%E6%B3%95/">算法</a> / <a href="/categories/%E7%AE%97%E6%B3%95/Leetcode/">Leetcode</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-31T10:18:00.000Z">2024-10-31</time></p><p class="title"><a href="/2024/10/31/linux_11_input_05_input_read_fasync/">linux_11_input_05_input_read_fasync</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/App/">App</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-31T07:41:00.000Z">2024-10-31</time></p><p class="title"><a href="/2024/10/31/linux_11_input_03_input_read_poll_more/">linux_11_input_03_input_read_poll_more</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/App/">App</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-31T07:40:00.000Z">2024-10-31</time></p><p class="title"><a href="/2024/10/31/linux_11_input_03_input_read_poll/">linux_11_input_03_input_read_poll</a></p><p class="categories"><a href="/categories/Linux/">Linux</a> / <a href="/categories/Linux/App/">App</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">十二月 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">十月 2024</span></span><span class="level-end"><span class="level-item tag">29</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">九月 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">六月 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">三月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">十二月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">十月 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">九月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">四月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Games101/"><span class="tag">Games101</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kunpeng/"><span class="tag">Kunpeng</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Leetcode/"><span class="tag">Leetcode</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">28</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E6%9C%AF/"><span class="tag">学术</span><span class="tag">7</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/head_circle.png" alt="BONESKEEP&#039; BLOG" height="28"></a><p class="is-size-7"><span>&copy; 2024 BONESKEEP</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">Open Source</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>