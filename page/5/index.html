<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>BONESKEEP&#039; BLOG</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="BONESKEEP&#039; BLOG"><meta name="msapplication-TileImage" content="/img/headlogo.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="BONESKEEP&#039; BLOG"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="BONESKEEP_BLOG"><meta property="og:type" content="blog"><meta property="og:title" content="BONESKEEP&#039; BLOG"><meta property="og:url" content="https://boneskeep.github.io/"><meta property="og:site_name" content="BONESKEEP&#039; BLOG"><meta property="og:description" content="BONESKEEP_BLOG"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://boneskeep.github.io/img/og_image.png"><meta property="article:author" content="BONESKEEP"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://boneskeep.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://boneskeep.github.io"},"headline":"BONESKEEP' BLOG","image":["https://boneskeep.github.io/img/og_image.png"],"author":{"@type":"Person","name":"BONESKEEP"},"publisher":{"@type":"Organization","name":"BONESKEEP' BLOG","logo":{"@type":"ImageObject","url":"https://boneskeep.github.io/img/head_circle.png"}},"description":"BONESKEEP_BLOG"}</script><link rel="icon" href="/img/headlogo.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/head_circle.png" alt="BONESKEEP&#039; BLOG" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives/">Archives</a><a class="navbar-item" href="/categories/">Categories</a><a class="navbar-item" href="/tags/">Tags</a><a class="navbar-item" href="/about/">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/BONESKEEP">GitHub</a><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/BONESKEEP/BONESKEEP.github.io"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-06-11T08:00:00.000Z" title="2024/6/11 16:00:00">2024-06-11</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:51:38.299Z" title="2024/12/4 15:51:38">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF/">学术</a><span> / </span><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/">三维重建</a></span><span class="level-item">1 小时读完 (大约10041个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/06/11/Advanced%20in%20Neural%20Rendering%20%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">Advanced in Neural Rendering 论文笔记</a></p><div class="content"><h3 id="Advanced-in-Neural-Rendering-论文笔记"><a href="#Advanced-in-Neural-Rendering-论文笔记" class="headerlink" title="Advanced in Neural Rendering 论文笔记"></a>Advanced in Neural Rendering 论文笔记</h3><hr>
<p>来源EuroGraphocs 的2022综述论文，2022年3月</p>
<hr>
<p><strong>ABSTRACT</strong></p>
<p>传统上，场景的合成图像使用 <strong>渲染算法</strong> （<strong>如光栅化或光线跟踪</strong>）生成的；</p>
<p>将特别定义的集合和材质属性表示作为输入。</p>
<p>这些输入定义了实际场景和渲染的内容，称为 <strong>场景表示</strong> ，场景由一个或多个目标组成。</p>
<p>示例场景表示是具有伴随纹理的 <strong>三角网格（艺术家创建）</strong>、<strong>点云（来自深度传感器）</strong>、<strong>体网格（来自CT扫描）</strong>或 <strong>隐式曲面函数（截断符号距离场）</strong>；</p>
<p>使用<strong>可微分渲染的损失</strong> 从观测中 重建这样的场景表示 被称为 <strong>逆图形学或逆渲染</strong>。</p>
<p>神经渲染结合了图形学和机器学习的思想，创建了从真实世界观测合成图像的算法；</p>
<p><strong>近年来该领域展示了 可将学习的组件 注入 渲染流水线的不同方法</strong></p>
<p>这篇报告侧重于 将经典渲染原理与学习的3D场景表示（神经场景表示）相结合的方法；</p>
<p>关键优点是在设计上的3D一致性，从而实现了捕获场景的新视点合成等应用；</p>
<p>除了 <strong>处理静态场景的方法</strong>外，还介绍了用于建模 <strong>非刚体变形目标</strong> 的神经场景表示以及场景编辑和合成；</p>
<p>这些方法大多时 <strong>场景特定</strong>的，但也讨论了 <strong>跨目标类进行泛化的技术</strong>，并可用于生成任务；</p>
<p>回顾这些最先进的方法外 还概述了 使用的基本概念和定义，最后讨论了公开挑战和社会影响。</p>
<hr>
<p><strong>1. INTRO</strong></p>
<p>传统计算机图形学可以生成高质量的可控图像，但是要基于物理定律并且场景的所有物理参数，比如 <strong>摄像机参数、照明和对象的材质</strong>都需要作为输入提供；</p>
<p>所以如果想要生成真实场景的可控图像，需要从现有的观测（如图像和视频）中估计这些物理属性，即逆渲染，具有挑战性，尤其是质量达到照片级时的合成图像。</p>
<p><strong>图像合成——需要物理参数——需要进行观测——观测后估计（逆渲染）——达成图像合成目标</strong></p>
<p><strong>挑战性在于逆渲染具有很大的挑战性！</strong></p>
<p>所以，神经渲染出现，允许场景的紧凑（compact）表示，而且可以通过 <strong>神经网络</strong>从现有观测中学习渲染。</p>
<p><strong>神经渲染的主要思想</strong>是：结合经典（基于物理的）计算机图形学的见解和深度学习的最新进展。</p>
<p><strong>神经渲染的目标</strong>：<strong>以可控的方式生成照片级真实感图像</strong>，例如新视点合成、重照明、场景变形和合成等。</p>
<p><strong>早期的神经绘制方法：</strong></p>
<p>使用神经网络将场景参数转换为输出图像，场景参数也可以直接作为<strong>一维输入</strong>给定，或者使用经典的图形学流水线生成<strong>二维输入</strong>。</p>
<p><strong>深度神经网络：</strong></p>
<p>通过对真实世界场景的观察进行训练，并学习对这些场景进行 <strong>建模和渲染</strong>。</p>
<p>具体来说：一个网络根据输入参数、模型架构和可训练参数定义了一个函数族。使用随机梯度下降，从这空间中找到最能解释由训练损失度量的训练集的函数。</p>
<p>神经绘制的目的是寻找控制参数与对应的输出图像之间的映射。</p>
<p>可以理解为一个复杂且具有挑战性的稀疏数据插值问题。因此，类似于经典的函数拟合，<strong>神经绘制需要在欠拟合和过拟合之间进行权衡</strong>。如果网络的表征能力不足，那么得到的图像质量就会很低，<strong>找到合适的网络架构</strong>本身就是一门艺术。在神经渲染的背景下，<strong>设计正确的物理驱动的归纳偏差往往需要强大的图形背景</strong>。</p>
<p>这方面的一个很好的例子是最近的神经渲染技术 <strong>NeRF</strong>，该技术试图<strong>通过仅学习3D场景表示</strong>并<strong>依赖计算机图形中的渲染函数</strong>进行<strong>监督</strong>来<strong>分离建模和渲染过程</strong>。</p>
<p><strong>神经辐射场（NeRF）</strong>使用多层感知器（MLP）来近似3D场景的辐射场和密度场。该学习的体表示可以使用解析可微分渲染（即体积分）从任何虚拟摄像头渲染。</p>
<p>对于训练，假设从多个摄像机视点观测场景。从这些训练视点，渲染估计的3D场景，并最小化渲染图像和观察图像之间的差异，根据这些观察结果训练网络。一旦训练完成，由神经网络近似的3D场景可以从新的视点进行渲染，从而实现可控合成。</p>
<p>与使用神经网络学习渲染函数的方法相反，<strong>NeRF</strong>在该方法中<strong>更明确地使用了计算机图形学的知识</strong>，由于（物理）归纳偏差，能够更好地概括新视图：<strong>场景密度和半径的中间3D结构化表示</strong>。因此，NeRF在3D空间中学习物理上有意义的颜色和密度值，物理激发的光线投射和体集成可以持续渲染到新视图中。</p>
<p>所取得的结果质量，以及方法的简单性，导致了该领域的“爆炸式”发展。已经取得了一些进步，提高了适用性，实现了可控性，动态变化场景的捕获以及训练和推理时间。</p>
<p>神经渲染发展非常快，在许多不同的维度上都取得了重大进展，因此对最近的方法及其应用领域进行了分类，以提供发展的简要概述。</p>
<hr>
<p><strong>2. Scope of This STAR</strong></p>
<p>在本报告中，重点介绍了将经典渲染与可学习3D表示相结合的高级神经渲染方法（见图2）。</p>
<p>先前关于神经渲染的STAR报告主要集中在此范式：<strong>2D神经渲染( 2D Neural Rendering )<strong>，也称为神经细化、神经重渲染或延迟神经渲染，是</strong>基于某个2D信号（例如一个语义标签或者一个栅格化的代理几何）输入</strong> 直接映射到输出图像——神经网络被训练用于渲染，例如使用<strong>经典的渲染器生成并学习在2D中渲染场景</strong>。</p>
<p>本报告聚焦于此范式：<strong>3D神经渲染</strong>学习在3D中表示一个场景，并使用来自计算机图形学的固定可微的渲染方案。例如NeRF，神经网络是有监督的，以表示特定场景的形状或外观，神经表示用略传统的图形“引擎”来呈现，通过分析定义，并不是被学习的。</p>
<p>通过设计，底层的神经3D表示是3D一致的，并且能够控制不同的场景参数。</p>
<p>在本报告中，全面概述了不同的场景表示，并详细介绍了从经典渲染流水线以及机器学习中借鉴的组件基本原理。进一步关注用神经辐射场以及体渲染的方法。然而，这里忽略主要在2D屏幕空间中推理的神经渲染方法，也不包括光线跟踪图像的神经超采样和去噪方法。</p>
<hr>
<p><strong>3. Fundamentals of Neural Rendering</strong></p>
<p>神经渲染管道从真实世界的图像中学习渲染和&#x2F;或表示场景，这些图像可以是无序的图像集，也可以是结构化的多视图图像或视频。</p>
<p>3D神经渲染的一个关键特性是该训练过程中相机捕获过程(即,投影和图像的形成)和3D场景表示的解耦。</p>
<p>这种解耦有许多优点，特别是在图像(例如,对于新颖的视点合成)的合成过程中，可以获得很高的3D一致性。</p>
<p>为了将投影等物理过程从三维场景表示中分离出来，三维神经绘制方法依赖于计算机图形学(例如,栅格化、点散布或体积积分)中已知的图像生成模型。</p>
<p>这些模型受到物理学的启发，特别是发射器的光线与场景以及相机本身的相互作用。这种光传输是使用渲染方程[ Kaj86 ]来描述的。</p>
<p>计算机图形学领域对此渲染方程提供了多种近似。</p>
<p>这些近似依赖于使用的场景表示，范围从经典的栅格化到路径追踪和体积积分。</p>
<p>3D神经渲染利用了这些渲染方法。</p>
<p>下面我们将详细介绍场景表示( 3.1节)以及常用神经绘制方法中使用的渲染方法( 3.2节)。</p>
<p>注意，<strong>为了从真实图像中学习</strong>，<strong>场景表示和渲染方法本身都必须是可微的</strong>( 3.3节)。</p>
<hr>
<p><strong>3.1. 场景表示</strong></p>
<p>几十年来，图形学界探索了各种表征，包括点云、隐式和参数曲面、网格和体积（见图）。</p>
<p>这些表示在 图形学领域有明确定义，但在当前神经渲染的文献中存在混淆，尤其是涉及到 <strong>隐式和显式曲面表示和体积表示</strong>。</p>
<p>通常，体表示可以表示曲面，反之亦然。</p>
<p>体表示存储 <strong>体特征</strong>，比如 <strong>密度、不透明度或占用率</strong>，也可以存储<strong>多维特征</strong>，如<strong>颜色或亮度</strong>。</p>
<p>与体表示不同，曲面表示存储<strong>目标曲面的特性</strong>。</p>
<p><strong>不能</strong>用于<strong>模拟体物质</strong>，如烟雾（除非是粗略近似值）。</p>
<p>对于曲面和体表示，都有<strong>连续和离散</strong>的对应项（见上图）。</p>
<p><strong>连续表示</strong>对于神经渲染方法特别有趣，因为它们<strong>可以提供解析梯度</strong>。</p>
<p>曲面表示（surface representation）有两种不同的方式——显式和隐式</p>
<p>在欧式空间中有显式曲面函数S<del>fexplicit</del>的定义（公式3.1 3.2 3.3）</p>
<p>体积表示（volume representation）定义了整个空间的属性（公式3.4）</p>
<p>以上所有表示，各自的功能领域都可以会被限制。具体见3.1。</p>
<p>在神经渲染的背景下，<strong>使用神经网络来近似（比如基于MLP的函数逼近器）表面或体积表示函数的场景表示</strong>称为<strong>神经场景表示</strong>。特别的，表面和体积表示都可以扩展以存储额外的信息，如颜色或与视角相关的辐射。</p>
<hr>
<p><strong>3.1.1. MLPs as Universal Function Approximators</strong></p>
<p>本节讨论了不同的基于MLP的函数逼近器，其构建了最近的神经曲面和体积表示的基础。</p>
<hr>
<p><strong>3.1.2. Representing Surfaces</strong></p>
<p>本节介绍了不同曲面表示方式，如点云、网格等。</p>
<ul>
<li><strong>点云(Point Clouds)</strong></li>
</ul>
<p><strong>点云</strong>是欧氏空间的元素集合。一个连续的曲面可以通过点云离散化，点云的每个元素代表曲面上的一个样本点( x , y , z)。对于每个点，可以存储额外的属性，如法线或颜色。</p>
<p><strong>具有法线特征</strong>的点云也被称为<strong>定向点云</strong>。除了简单的点可以看成无穷小的表面面片外，还可以使用半径为(表示位于下垫面切平面上的2D圆盘)的定向点云。这种表象被称为表面元素，别名面元[ PZvBG00 ]。</p>
<p>它们经常被用于计算机图形学中，用于从模拟中渲染点云或粒子。这样的面元的渲染被称为抛雪球，最近的工作表明它是<strong>可微的</strong>。</p>
<p>使用这种可微的渲染管线，可以直接反向传播到点云位置以及伴随特征(例如,半径或颜色)。</p>
<p>在Neural基于点的图形学和SynSin中，可学习的特征<strong>被附加到</strong>能够<strong>存储</strong>关于<strong>实际表面的外观和形状</strong>的丰富<strong>信息</strong>的<strong>点</strong>上。</p>
<p>在ADOP中，这些可学习的特征由一个可以解释视图依赖效应的MLP来解释。值得注意的是，对于<strong>离散</strong>位置，也可以使用MLP来预测特征，而不是存储特定点的显式特征。</p>
<p>如前所述，点云是欧氏空间的元素集合，因此，除了曲面，它们<strong>还可以表示体</strong>(例如,存储额外的不透明度或密度值)。对每个点使用一个半径自然会导致一个完整的基于球的公式。</p>
<ul>
<li><strong>网格(Meshes)</strong></li>
</ul>
<p><strong>网格</strong>。多边形网格表示曲面的分段线性逼近。特别地，三角网格和四边形网格在计算机图形学中被用作表面事实上的标准表示。</p>
<p>优化了图形流水线和图形加速器( GPU )，以每秒处理和光栅化数十亿个三角形。</p>
<p>大多数图形编辑工具使用<strong>三角形网格</strong>，这使得这种表示对于任何内容创建管道都很重要。为了与这些管线直接兼容，许多”经典”的逆向图形和神经绘制方法都使用了这种基本的曲面表示。</p>
<p>使用<strong>可微</strong>的渲染器，可以<strong>优化顶点位置和顶点属性</strong>(例如,颜色)，以再现一幅图像。</p>
<p>神经网络可以被训练来<strong>预测顶点的位置</strong>，例如，预测动态变化的曲面。而不是使用顶点属性，在三角形内存储表面属性的常用策略是<strong>纹理贴图</strong>。</p>
<p>2D纹理坐标是贴在网格顶点上的，它参考了纹理图像中的一个位置。利用重心插值，可以计算三角形内任意一点的纹理坐标，并利用双线性插值从纹理中检索属性。纹理的概念也被集成到了标准的图形流水线中，增加了一些额外的功能，如分级细化，它需要妥善处理纹理( c.f. ,采样定理)的采样。</p>
<p><strong>延迟神经绘制</strong>( Deferred Neural Rendering )使用包含可学习的视点相关特征的纹理，即所谓的<strong>神经纹理</strong>。具体来说，一个粗网格被用作底层的3D表示，以光栅化这些神经纹理。神经网络在图像空间中解释这些光栅化的特征。注意到该网络可以是一个像素级的MLP，那么神经纹理就代表了表面辐亮度。</p>
<p>相对于使用离散纹理，可以使用连续纹理。纹理场的作者提出使用MLP来预测每个表面点的颜色值。在<strong>神经反射场纹理( NeRF-Tex )</strong> 中，将NeRF的思想与使用2D神经纹理和底层3D网格的思想相结合。NeRF - Tex是<strong>基于用户定义的参数来控制外观</strong>，因此<strong>可以被</strong>艺术家<strong>编辑</strong>。</p>
<ul>
<li><strong>隐式曲面(Implicit Surfaces)</strong></li>
</ul>
<p><strong>隐式曲面</strong>。隐式曲面将曲面定义为函数的零水平集，见公式3 。最常用的隐式曲面表示方法是符号距离函数( SDF )。这些SDF表示被用于许多使用体积融合增量重建静态或动态物体表面的3D扫描技术中。<strong>隐式曲面表示</strong>提供了许多优点，因为它们<strong>避免了定义网格模板的要求</strong>，因此<strong>能够在动态场景中表示拓扑未知或拓扑变化的物体</strong>。</p>
<p>上述体积融合方法使用<strong>离散</strong>(截断)的符号距离函数，即使用<strong>包含符号距离值</strong>的3D网格。Hoppe等人提出了分段线性函数来对输入曲面点样本的符号距离函数进行建模。Carr等人的开创性工作使用径向基函数网络代替。这个径向基函数网络代表了一个连续的隐式曲面函数，可以看作是第一个”神经”隐式曲面表示。</p>
<p><strong>最近的</strong>神经隐式曲面表示是<strong>基于坐标的多层感知器</strong>( MLPs )，见3.1.1节。这种表示方法在<strong>神经场景的表示和渲染中</strong>得到了<strong>广泛应用</strong>。它们在[ PFS⋅19 , CZ19]中同时被提出<strong>用于形状建模</strong>，其中MLP架构用于将连续坐标映射到符号距离值。这种坐标网络表示的信号的保真度，或者说神经隐式表示，<strong>主要受限于网络的容量</strong>。因此，与上述其他表示法相比，隐式曲面<strong>在记忆效率方面具有潜在的优势</strong>，并且作为一种<strong>连续表示法</strong>，<strong>理论上</strong>可以<strong>表示无限分辨率的几何图形</strong>。</p>
<p>最初的建议接踵而至，产生了广泛的热情，随后有了各种针对不同侧重点的改进，包括<strong>训练方案、利用全局-局部上下文、采用特定参数化或空间分区</strong>。<strong>（创新点）</strong></p>
<p>由于<strong>不需要预先定义网格模板或对象拓扑结构</strong>，<strong>神经隐式曲面</strong>非常<strong>适合对不同拓扑结构的对象</strong>进行建模。</p>
<p><strong>利用反向传播</strong>可以<strong>计算输出相对于输入坐标的解析梯度</strong>。这使得除了其他几何激励的正则化器外，在梯度上实现正则化项成为可能。</p>
<p>这些表示可以扩展到对场景的辐亮度进行编码。这对于神经渲染是有用的，在这里我们<strong>希望</strong>场景表示能够同时<strong>编码场景的几何和外观</strong>。</p>
<hr>
<p><strong>3.1.3. Representing Volumes</strong></p>
<p>本节介绍了不同体积表示方式，如体素网格、神经体积表示等。</p>
<ul>
<li><strong>体素网格( Voxel Grids )</strong></li>
</ul>
<p>体素网格( Voxel Grids )。作为<strong>R^3^</strong>中的像素等价体，体素通常被用来<strong>表示体积</strong>。</p>
<p>它们可以<strong>存储几何占有率</strong>，或者存储<strong>具有体积效应</strong>(如<strong>透明度</strong>)的场景的密度值。</p>
<p>此外，场景的外观可以存储。</p>
<p>使用三线性插值，这些体属性可以在体素网格内的任意点访问。这种插值方法特别适用于光线投射等基于样本的绘制方法。</p>
<p>在存储的属性可以具有特定的语义(例如,占有率)的同时，属性也可以被学习。</p>
<p>西茨曼等人提出使用<strong>DeepVoxels</strong>，将<strong>特征</strong>存储在体素网格中。光线<strong>投射渲染</strong>程序后的<strong>特征的积累和解释</strong>是使用深度<strong>神经网络</strong>完成的。这些DeepVoxels可以看作是<strong>体积神经纹理</strong>，可以直接使用<strong>反向传播</strong>进行优化。</p>
<p>虽然基于稠密体素的表示可以快速查询，但它们的内存效率很低，并且3D CNNs可能在这些体上运行，计算量很大。</p>
<p><strong>八叉树</strong>数据结构（用于<strong>描述空间</strong>的树状数据结构）可以用稀疏的方式来表示体积。八叉树上的稀疏3D卷积可以帮助缓解一些问题，但这些紧凑的数据结构不能轻易地在工作中更新。因此，它们很难融入到学习框架中。</p>
<p>其他<strong>减轻密集体素网格内存</strong>挑战的方法包括使用对象特定形状模板，多平面或多球体图像，它们都旨在使用<strong>稀疏近似</strong>来表示体素网格。</p>
<ul>
<li><strong>神经体积表征(Neural Volumetric Representations)</strong></li>
</ul>
<p>神经体积表征(Neural Volumetric Representations)。特征或其他感兴趣的量也可以<strong>使用神经网络</strong>来定义，类似于<strong>神经隐式曲面</strong>(见3.1.2节)。MLP网络体系结构可以用于参数化体积，与显式体素网格相比，MLP网络体系结构具有更高的存储效率。</p>
<p>这些表示可能仍然成本巨大，需要<strong>根据基础网络的大小进行采样</strong>，因为对于每个样本，必须计算通过网络的整个前馈。大多数方法可以粗略地分类为使用全局或局部网络。</p>
<p>同时使用网格和神经网络的混合表示在计算效率和内存效率之间做出了权衡。</p>
<p>与神经隐式曲面类似，神经体积表示<strong>允许计算解析梯度</strong>，这<strong>已被用于定义正则化项</strong>。</p>
<p>BACON引入了<strong>基于带限坐标</strong>的网络，学习了<strong>表面的光滑多尺度分解</strong>。</p>
<ul>
<li><strong>总的注释(General remark)</strong></li>
</ul>
<p>总的注释(General remark)：</p>
<p>使用<strong>基于坐标</strong>的神经网络对场景<strong>进行体积建模</strong>(如<strong>NeRF</strong>)，表面上类似于使用坐标网络对表面进行隐式建模(如在神经隐式曲面中)。</p>
<p>然而，类NeRF的<strong>体表示并不一定是隐式</strong>的——因为网络的输出是<strong>密度和颜色</strong>，场景的几何结构是由<strong>网络显式</strong>地，而不是隐式地参数化的。</p>
<p>尽管如此，这些模型在文献中仍然被称为”隐式”是很常见的，这可能是由于场景的<strong>几何形状</strong>是由<strong>神经网络</strong>（SDF文献中使用了一个不同的”隐式”定义）的**权重”隐式”**定义的。</p>
<p>同时注意到，这是一个与深度学习和统计社区通常使用的”隐式”不同的定义，其中**”隐式”<strong>通常是</strong>指模型的输出被隐式地定义为动态系统的固定点<strong>，其</strong>梯度使用隐函数定理计算**。</p>
<hr>
<p><strong>3.2. 可微图像的生成 Differentiable Image Formation</strong></p>
<p>前面几节中的场景表示允许我们表示场景的3D几何和外观。</p>
<p>下一步，我们将描述<strong>如何通过渲染从这些场景表示中生成图像</strong>。</p>
<p>将三维场景渲染成二维图像平面的一般方法有两种：光线投射和栅格化，见图4。</p>
<p>也可以通过<strong>定义场景中的相机</strong>来<strong>计算场景的渲染图像</strong>。</p>
<p>大多数方法使用<strong>针孔相机</strong>，其中所有相机**光线都通过空间中的单点(焦点)**。</p>
<p>对于<strong>给定的相机</strong>，可以<strong>将来自相机原点的光线投射到场景中</strong>，以便<strong>计算渲染图像</strong>。</p>
<p>(a) <strong>正向渲染</strong>(例如,光栅化)——通过 <strong>将三维表示投影到图像平面上</strong> 生成图像。</p>
<p>(b) <strong>光线投射</strong>( Ray Casting )——通过<strong>投射观看光线</strong>，<strong>采样3D表示</strong>并<strong>累加生成图像</strong>。</p>
<p><strong>上图描述</strong>：</p>
<p>对于<strong>显式曲面表示</strong>，曲面是直接可索引的。这使得我们可以使用前向渲染方法，将表面投影到图像平面，并相应地设置一个像素(例如,使用栅格化或点散布)。<strong>隐式表面表示和体积表示</strong>，<strong>不提供</strong>允许前向渲染的表面的<strong>直接信息</strong>，相反，从虚拟相机看到的3D空间必须采样以生成图像(例如,使用<strong>光线行进</strong>法)。</p>
<p><strong>光线投射</strong>。在针孔模型中，<strong>基本截距定理</strong>可以用来描述<strong>一个点p∈R^3^在三维中如何被投影到图像平面中的正确位置q∈R^2^</strong>。</p>
<p>它被定义为一个非内射函数，并且很难求逆，这使得它在三维重建问题中处于核心地位。</p>
<p><strong>Pinhole模型</strong>对该投影具有单一参数矩阵：<strong>本征矩阵K</strong>包含经像素尺寸归一化的<strong>焦距f &#x3D; [α<del>x</del> , α<del>y</del>]<strong>，</strong>轴偏斜度γ</strong>和<strong>中心点c</strong> &#x3D; [c<del>x</del> , c<del>y</del>]。利用截距定理并假设齐次坐标p′&#x3D; [ x , y , z , 1]，我们发现投影坐标为q′&#x3D; K · p′，有</p>
<p>这假设投影的中心在坐标原点，并且相机是轴对齐的。为了<strong>推广到任意相机位置</strong>，可以使用一个<strong>外部矩阵R</strong>。这个均匀的4 × 4矩阵E是由</p>
<p>其中<strong>R是旋转矩阵</strong>，t是平移向量，使得<strong>R · pw + t &#x3D; pc</strong>，这里我们用<strong>pw表示世界坐标中的点</strong>，pc表示相机坐标中的点。R和t的这种定义在计算机视觉(例如, OpenCV使用的)中很常见，被称为”世界到世界”映射，而在计算机图形学(例如,在OpenGL中)中，类似的逆”凸轮到世界”映射更普遍。假设” worldto-cam “约定，利用齐次坐标，我们可以写出<strong>pw到qp的全投影</strong>为：</p>
<p>如果使用’ cam-to-world ‘约定，光线投射也同样方便。由于深度模糊，这些方程是非单射的，但它们非常<strong>适合于自动微分</strong>，并且可以在<strong>图像形成模型</strong>中进行端到端的优化。</p>
<p>为了正确地对当前的相机进行建模，还必须考虑另外一个部件：<strong>镜头</strong>。</p>
<p>抛开在图像形成过程中必须建模的景深或运动模糊等影响，它们<strong>会给投影函数添加失真影响</strong>。不幸的是，没有一个单一的、简单的模型来捕获所有不同的镜头效果。</p>
<p>标定包，如OpenCV中的标定包，通常实现最多12个畸变参数的模型。它们是通过五次多项式建模的，因此不是平凡可逆的(这是光线投射相对于点投影所需要的)。</p>
<p><strong>更现代的摄像机</strong>标定方法使用了更多的参数，并达到了更高的精度，并且可以实现可逆和可微。</p>
<ul>
<li><strong>光栅化&#x2F;栅格化 Rasterization</strong></li>
</ul>
<p>光栅化。光线投射的一种替代方法是将几何图元栅格化。</p>
<p>该技术并不试图模仿真实世界的图像形成过程，而是利用物体的几何特性来快速生成图像。</p>
<p>它主要用于网格，由顶点v和面f的集合描述，连接顶点的三元组或四元组来定义曲面。</p>
<p>一个基本的见解是，3D中的几何操作可以单独处理顶点：例如，我们可以使用相同的外部矩阵E将世界中的每个点变换到相机坐标系中。</p>
<p>经过这种变换后，<strong>可以剔除视锥体之外的点或法线方向错误的点</strong>，以减少下一步需要处理的点和面的数量。</p>
<p>通过使用如上所述的内蕴矩阵K，可以再次平凡地找到投影到图像坐标的其余点的位置。</p>
<p>利用人脸信息可以对人脸图元进行深度插值，最上层的人脸可以存储在一个零缓冲器中。</p>
<p>这种实现投影的方式<strong>往往比光线投射更快</strong>：它主要根据场景中<strong>可见顶点的数量</strong>进行缩放，而光线投射则根据<strong>像素的数量和要与之相交的图元的数量</strong>进行缩放。然而，使用(例如,光影、阴影、反射等)来捕获某些效应是比较困难的。它可以通过”软”栅格化使其具有<strong>可微性</strong>。例如，在[ LLCL19 , RRN⋅20]中已经实现。</p>
<hr>
<p><strong>3.2.1. 曲面渲染 Surface Rendering</strong></p>
<ul>
<li><strong>点云渲染 Point Cloud Rendering</strong></li>
</ul>
<p>点云渲染。点云是<strong>连续曲面或体积</strong>的<strong>离散样本</strong>。</p>
<p>点云渲染对应于从不规则分布的离散样本中重建连续信号，例如连续表面的出现，然后在每个像素位置的图像空间中重采样重建信号。</p>
<p>这个过程可以通过<strong>两种不同的方式</strong>来完成：</p>
<p>第一种方法<strong>基于经典信号处理理论</strong>，可以看作是一个”软”点抛雪球（Splatting 喷溅）(类似于下面网格渲染部分中的软光栅化器)。它首先利用连续的局部重构核r ( · )构造连续信号，即f &#x3D;∑f i ( pi )。</p>
<h6 id="本质上，这种方法相当于将离散样本与一些局部确定性模糊核混合，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。"><a href="#本质上，这种方法相当于将离散样本与一些局部确定性模糊核混合，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。" class="headerlink" title="本质上，这种方法相当于将离散样本与一些局部确定性模糊核混合，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。"></a>本质上，这种方法相当于<strong>将离散样本与一些局部确定性模糊核混合</strong>，例如EWA抛雪球，它是一种空间变化的重构核，旨在最小化混叠。在神经绘制中，离散样本可以存储一些可学习的特征。</h6><p>相应地，前述步骤有效地将个体特征投影并融合到2D特征图中。</p>
<p>如果特征具有预定义的语义(例如,颜色,法线)，可以使用固定的着色函数或BRDF来生成最终的图像。</p>
<p>如果特征是学习到的神经描述符，则部署2D神经网络将2D特征图转换为RGB图像。</p>
<h6 id="最近采用这种方法的神经点绘制方法包括Sin-Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。"><a href="#最近采用这种方法的神经点绘制方法包括Sin-Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。" class="headerlink" title="最近采用这种方法的神经点绘制方法包括Sin Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。"></a>最近采用这种方法的神经点绘制方法包括Sin Syn和Pulsar。为了性能的原因，他们在混合步骤中使用空间不变和各向同性的核。</h6><p>虽然这些<strong>简化的内核会导致渲染伪影</strong>，如空洞，模糊边缘和锯齿，但这些<strong>伪影可以在神经着色</strong>步骤中<strong>得到补偿</strong>，并且在Pulsar的情况下，可以通过优化半径来实现。[ KPLD21 ]还使用了相机选择和概率深度测试的策略，并能够在该框架中处理IBR、风格化和协调。</p>
<p>除了<strong>软点绘制方法</strong>外，还可以使用传统的OpenGL或directx技术的<strong>点渲染器</strong>。在这里，每个点被投影到单个像素(或小面积的像素)，从而得到一个稀疏的特征图。可以使用深度神经网络直接在图像空间中重构信号。注意，这种朴素的渲染方法不提供关于点位置p的梯度，只允许区分渲染函数w . r . t . (神经)特征。相比之下，软点散布方法通过重构核r ( p )提供点位置梯度。</p>
<h6 id="然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。-YSW※19b-通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献-RFS21b-中被采用，以联合优化几何和相机姿态来进行新的视图合成。"><a href="#然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。-YSW※19b-通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献-RFS21b-中被采用，以联合优化几何和相机姿态来进行新的视图合成。" class="headerlink" title="然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。[ YSW※19b ]通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献[ RFS21b ]中被采用，以联合优化几何和相机姿态来进行新的视图合成。"></a>然而，即使在这种情况下，梯度在空间上也被限制在局部重建的支持范围内。[ YSW※19b ]通过使用有限差分来近似梯度来解决这个问题，并成功地将渲染器应用于表面去噪、风格化和多视图形状重建。这一思想在文献[ RFS21b ]中被采用，以联合优化几何和相机姿态来进行新的视图合成。</h6><ul>
<li><strong>网格渲染 Mesh Rendering</strong></li>
</ul>
<p>有许多通用的渲染器<strong>允许网格被栅格化或以可微的方式渲染</strong>。在可微网格栅格化中，Loper和Black开发了一个可微的渲染框架，称为OpenDR，它<strong>近似一个主要的渲染器</strong>，并<strong>通过自动微分来计算梯度</strong>。</p>
<p><strong>神经网格渲染器( NMR )</strong> 使用可见度变化的手工函数来近似光栅化操作的向后梯度。</p>
<h6 id="文献-LTJ18-提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人-PBDCO19-提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，-LLCL19-提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。"><a href="#文献-LTJ18-提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人-PBDCO19-提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，-LLCL19-提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。" class="headerlink" title="文献[ LTJ18 ]提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人[ PBDCO19 ]提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，[ LLCL19 ]提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。"></a>文献[ LTJ18 ]提出了无固定职业的摄影师，一种利用图像滤波器进行网格几何处理的解析可微渲染器。彼得森等人[ PBDCO19 ]提出了Pix2Vex，一种通过邻近三角形的软混合方案的C∞可微渲染器，[ LLCL19 ]提出了Soft光栅化程序，它渲染和聚合网格三角形的概率映射，允许梯度从渲染的像素流向被遮挡的和远程的顶点。</h6><h6 id="虽然大多数栅格化器只支持基于直接光照的渲染，但-LHL⋅21-也支持软阴影的可微渲染。在基于物理的绘制领域，-LADL18a-和-ALKN19-引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba-2-NDVZJ19-和Taichi-HLA-19-HAL-20-是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。"><a href="#虽然大多数栅格化器只支持基于直接光照的渲染，但-LHL⋅21-也支持软阴影的可微渲染。在基于物理的绘制领域，-LADL18a-和-ALKN19-引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba-2-NDVZJ19-和Taichi-HLA-19-HAL-20-是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。" class="headerlink" title="虽然大多数栅格化器只支持基于直接光照的渲染，但[ LHL⋅21 ]也支持软阴影的可微渲染。在基于物理的绘制领域，[ LADL18a ]和[ ALKN19 ]引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba 2 [ NDVZJ19 ]和Taichi [ HLA * 19 , HAL * 20]是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。"></a>虽然<strong>大多数栅格化器只支持基于直接光照的渲染</strong>，但[ LHL⋅21 ]也支持软阴影的可微渲染。在基于物理的绘制领域，[ LADL18a ]和[ ALKN19 ]引入了可微分的光线示踪器，实现了基于物理的绘制效果的可微分性，处理了相机位置、光照和纹理。此外，Mitsuba 2 [ NDVZJ19 ]和Taichi [ HLA * 19 , HAL * 20]是通用的基于物理的渲染器，通过自动区分支持可微的网格渲染，以及其他许多图形技术。</h6><ul>
<li><strong>神经隐式曲面渲染 Neural Implicit Surface Rendering</strong></li>
</ul>
<p>当<strong>输入观测为2D图像</strong>时，实现隐式曲面的网络不仅<strong>产生几何相关的量</strong>，即符号距离值，而且<strong>产生外观相关的量</strong>。</p>
<p>一个隐式可微渲染器可以通过首先使用神经隐式函数的几何分支找到一条观察光线与曲面的交点，然后从外观分支获得该点的RGB值来实现。曲面求交的搜索通常是基于球面追踪算法[ Har96 ]的一些变体。</p>
<p>球面跟踪在视线方向上从相机中心对三维空间进行迭代采样，直到到达表面。<strong>球面追踪是一种优化的射线追踪方法</strong>，通过前一位置采样的SDF值来调整步长，但这种迭代策略仍然会带来计算上的开销。泷川等人[ TLY※21 ]通过将<strong>光线跟踪算法与稀疏八叉树数据结构相适应</strong>，<strong>提高了渲染性能</strong>。</p>
<p>在二维监督的联合几何和外观估计的隐式曲面渲染中，一个常见的问题是几何和外观的模糊性。在[ NMOG20 , YKM * 20 , KJJ * 21 , BKW21]中，<strong>从2D图像中提取前景掩码</strong>，为几何分支<strong>提供额外的监督信号</strong>。最近，[ OPG21 ]和[ YGKL21b ]通过<strong>将表面函数转化为体渲染公式</strong>(下面介绍)来解决这个问题；另一方面[ ZYQ21 ]利用现成的深度估计方法生成伪地面真值符号距离值来辅助几何分支的训练。</p>
<hr>
<p><strong>3.2.2 体积渲染 Volumetric Rendering</strong></p>
<p>体积渲染是<strong>基于光线投射</strong>的，并且已经被证明在神经渲染，特别是在从<strong>多视图输入数据</strong>中<strong>学习</strong>场景表示方面是有效的。具体来说，这个场景表示为<strong>体积密度或占有率</strong>的连续场，而不是硬表面的集合。</p>
<p>这意味着<strong>光线</strong>在<strong>空间中的每一点</strong>都有一定的概率<strong>与场景内容发生交互</strong>，而不是二进制的相交事件。</p>
<p>这种连续模型可以很好地作为机器学习管道的可微渲染框架，这些机器学习管道<strong>严重依赖良好行为梯度的存在</strong>进行优化。</p>
<p>虽然完全通用的体积渲染确实考虑了光线可以从一个体粒子反射出去的”散射”事件，但我们将这一总结限制在用于<strong>视图合成</strong>的神经体积渲染方法常用的基本模型中，它只考虑了光线被<strong>一个体粒子</strong>发射或阻挡的”发射”和”吸收”事件。</p>
<p><strong>给定</strong>一组像素<strong>坐标</strong>，我们可以使用先前描述的相机模型<strong>计算通过原点p和方向ω<del>o</del>的3D空间的相应光线</strong>。沿着这条射线的入射光可以用一个简单的发射&#x2F;吸收模型定义为</p>
<p>其中σ为点的体积密度，L<del>e</del>为点和方向的出射光，透射率T为嵌套积分表达式</p>
<p><strong>密度</strong>表示<strong>光线在特定点与场景的体积”介质”相互作用的微分概率</strong>，而<strong>透射率</strong>描述了<strong>光线从点p + tω<del>o</del>回到相机时被衰减的程度</strong>。</p>
<p>这些表达式只能针对简单的密度场和色场进行解析计算。在实际应用中，我们通常<strong>用求积来近似积分</strong>，其中σ和Le被假设为在N个区间{ [ ti - 1，ti ) } ^N^<del>i&#x3D;1</del>内的分段常数，该区间划分了射线的长度：</p>
<p>对于这个近似的完整推导，我们参考Max和Chen 。注意，当写成这种形式时，逼近L的表达式恰好对应于<strong>由后向前合成颜色L^(i)^<del>e</del>的alpha</strong>。</p>
<p><strong>NeRF和相关方法</strong>使用可微体积渲染<strong>将场景表示投影到2D图像中</strong>。这使得这些方法可以<strong>在”逆向渲染”框架中使用</strong>，其中三维或更高维的场景表示是从2D图像中估计出来的。体绘制需要沿一条射线处理多个样本，每个样本都需要通过网络进行完整的前向传递。</p>
<p>最近的工作<strong>提出了增强数据结构、重要性采样、快速集成</strong>等策略来<strong>加快渲染速度</strong>，尽管这些方法的<strong>训练时间仍然较慢</strong>。<strong>自适应坐标网络</strong>使用多分辨率网络结构加速训练，在训练阶段通过以最优和有效的方式分配可用的网络容量来优化网络结构。</p>
<hr>
<p><strong>3.3. 优化 Optimization</strong></p>
<p><strong>训练</strong>神经网络的<strong>核心</strong>是非线性优化，其目的是<strong>应用训练集的约束</strong>，以<strong>获得一组神经网络权值</strong>。</p>
<p>因此，由神经网络逼近的函数能够拟合给定的训练数据。通常，神经网络的优化是基于梯度的；更具体地说，使用了SGD的变体，如Momentum或Adam，其中梯度是通过反向传播算法获得的。</p>
<p>在神经渲染的背景下，神经网络实现了3D场景表示，训练数据由场景的2D观测值组成。使用神经场景表示的可微渲染得到的渲染结果与使用给定的观察进行了比较。</p>
<p>这些重建损失可以用每个像素的L1或L2项来实现，也可以使用基于感知的甚至基于判别器的损失公式。然而，关键是这些损失直接与各自的可微渲染公式耦合，以更新场景表示，见3.1节。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-06-11T08:00:00.000Z" title="2024/6/11 16:00:00">2024-06-11</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:51:37.466Z" title="2024/12/4 15:51:37">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF/">学术</a><span> / </span><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF/NeRF/">NeRF</a></span><span class="level-item">9 分钟读完 (大约1369个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/06/11/NeRF%E5%85%A5%E9%97%A8%E5%8F%8A%E8%BF%9B%E9%98%B6%E2%80%94%E2%80%94%E4%B8%AD%E6%81%A9%E5%AE%9E%E9%AA%8C%E5%AE%A4/">NeRF入门及进阶——中恩实验室</a></p><div class="content"><p><strong>NeRF原理快速入门</strong></p>
<p><strong>1 关于渲染与反渲染（图形学）</strong></p>
<p>渲染rendering：基于一个三维模型以及材质和光照的信息，通过一个特定的视角，将物体渲染成一个精美的画面。（上图）</p>
<p>反渲染inverse rendering（三维重建）：通过渲染出来的图像，重新得到三维模型。一般会用网格、点云或者体素的方式表现。</p>
<p><strong>2 NeRF</strong></p>
<p>NeRF也是三维重建的一种技术，但是和以往技术不同：</p>
<ul>
<li>以往技术：通过图片重建模型</li>
<li>NeRF：通过神经隐式重建模型</li>
</ul>
<p>神经隐式：将三维模型的信息存在于神经网络中，输入相机位姿，输出图片的RGB和不透明度alpha。</p>
<p>可简单理解为：使用一个NeRF神经网络采取体积雾（带不透明度的点云）的渲染方式，通过已知视角的图片进行训练，然后输入其他相机视角的参数，从而预测出未出现视角的图片。</p>
<p>此时三维模型的信息存储在NeRF神经网络之中，所以是一种隐式的表示方法，而不是像点云、网格显示表示。</p>
<p>一个NeRF网络模型只能存储一个三维物体或场景的信息（单对单训练模型），现在有一些相关工作：希望网络可以同时使用在多个场景和多个三维模型（泛化）。</p>
<p><strong>3 原理</strong></p>
<p><strong>3.1 输入：相机位姿——5D Input</strong></p>
<p>Position 相机位置：(x，y，z)</p>
<p>Direction 相机方向：(θ，φ)，这是球坐标的表现方法，只要两个参数，方位角和俯仰角，代表的是相机位置与成像平面所对应像素连线发出的“光线”射线的方位，类似与Yaw，Pitch，Roll中的Yaw和Pitch，但不包含Roll。</p>
<p><strong>注：</strong></p>
<ol>
<li>θ是点在xOy平面上的投影 与 原点的连线 和 x轴正方向所成夹角，也就是一般说的极坐标的θ，取值范围为[0,2π)或[0,2π]。</li>
<li>φ是点与原点所成连线和z轴正半轴所成夹角，取值范围为[-π,π]（闭区间，否则顶点取不到）。</li>
<li>实际上在网络输入的仍然是三维向量，来表示射线方向xyz，和相机位置的xyz不同。</li>
</ol>
<p><strong>3.2 神经网络 F(θ)</strong></p>
<p>最后目的就是把在这个相机角度下所拍摄到的图片的成像内容给输出出来。</p>
<p>一个像素点对应的射线和相机位置就是一个训练资料，一个神经网络模型只需要40mb的大小就可以存储下3D模型内容。</p>
<p><strong>3.3 输出：一根射线上一组采样点的属性——4D Output</strong></p>
<p>输出像素点对应射线上的一组采样点颜色值(r,g,b)和不透明度值(α)。</p>
<p>如图b，在一定范围内取了一组采样点（橙色小点，每个点带有rgba的值），代码中使用near和far参数来表示光线能到达的最近及最远点。</p>
<p>在射线方向上对采样渲染点进行积分得到确切颜色值（通过体积雾渲染的方式，图c，d），在第一次出现波峰对该像素点的着色影响最大。（所以NeRF渲染出来的物体有一种看起来像雾一样的感觉）</p>
<p><strong>注：</strong></p>
<ol>
<li>Density密度值&#x2F;不透明度：α 该值与观测角度无关</li>
</ol>
<p><strong>3.4 位置编码γ(x,y,z) Positional Encoding</strong></p>
<p>将图像中的高频信息体现出来，可以大幅度提高图片的细节质量</p>
<p>NeRF中的位置编码是通过concat 的方式将信息融合起来</p>
<p>F<del>θ</del>&#x3D;F‘<del>θ</del> ○ γ</p>
<p>γ(p) &#x3D; (sin(2^0^πp),cos(2^0^πp),…,sin(2^L-1^πp),cos(2^L-1^πp)).</p>
<p>sin和cos两个为一项，空间坐标中用10项，即20维离开分别表示(x,y,z)位置编码</p>
<p>相机方向用4项，及8维来分别表示(x,y,z)位置编码</p>
<p><strong>4 网络图</strong></p>
<p>用全连接层一直连得到最终结果</p>
<p><strong>5 Volume Render部分</strong></p>
<p>采用一种体积雾的渲染方式，在获取一定范围采样点的(r,g,b,a)后需要再进行特定积分运算，最终得到对应像素最终的(r,g,b,a)，在训练时通过光线采样点积分得到的像素值</p>
<p>存在一个问题：光线穿透不透明的物体时 物体是存在前面和后面 后面的内容是不应该被成像出来的（物体不透明）</p>
<p>解决方式：通过一种特定的积分方式，主要是<strong>计算了第一个波峰</strong>所对应的颜色和不透明度值来产生对应的画面中的像素值</p>
<p><strong>总结</strong></p>
<p>通过一组图片（和对应的相机位姿）去训练一个网络，网络存储对应模型信息，再输入一个不一样视角的相机参数，就能呈现我们想要看到的物体的一个角度的成像。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-06-11T08:00:00.000Z" title="2024/6/11 16:00:00">2024-06-11</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:51:40.956Z" title="2024/12/4 15:51:40">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF/">学术</a><span> / </span><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/">三维重建</a></span><span class="level-item">31 分钟读完 (大约4606个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/06/11/%E7%A5%9E%E7%BB%8F%E6%B8%B2%E6%9F%93%EF%BC%9A%E5%9B%BE%E5%BD%A2%E5%AD%A6%20+%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经渲染：图形学 + 深度学习</a></p><div class="content"><h3 id="神经渲染：图形学-深度学习"><a href="#神经渲染：图形学-深度学习" class="headerlink" title="神经渲染：图形学 + 深度学习"></a>神经渲染：图形学 + 深度学习</h3><hr>
<p>利用深度学习和图形学技术，实现高质量、高效率、高灵活性的图像合成和渲染的方法，并且能够对图像进行编辑，从而实现多种应用，主要应用于影视动画、游戏开发、虚拟现实、自动驾驶等领域</p>
<p><strong>原理</strong></p>
<p>利用深度学习模型来模拟图形学渲染的过程，实现从输入到输出的端到端映射。</p>
<p>神经渲染是基于数据驱动和统计推断的概率模型，对场景中的信息进行隐式的表示和学习，通过大量的数据来模拟渲染过程；</p>
<p>传统图形学渲染需要基于物理规律和数学模型的确定性算法，需要对场景中的几何、材质和光照等要素进行精确的描述和计算。</p>
<p><strong>主要流程</strong></p>
<p><strong>「空间表示」：</strong>指将三维空间中的信息以一种适合于深度学习模型处理的方式进行编码和存储。常见的空间表示方法有体素voxel、点云point cloud、网格mesh、隐函数implicit function等。</p>
<p><strong>「几何重建」：</strong>几何重建是指<strong>根据输入的二维图像或视频，恢复出三维空间中的几何结构</strong>。常见的几何重建方法有多视图立体（multi-view stereo）、结构光（structured light）、深度相机（depth camera）等。</p>
<p><strong>「光照模拟」：</strong>光照模拟是指<strong>根据输入或预设的光照条件，计算出三维空间中各个位置的光强度和颜色</strong>。常见的光照模拟方法有光线追踪（ray tracing）、光线投射（ray casting）、辐射度（radiosity）等。</p>
<p><strong>「视觉合成」：</strong>视觉合成是<strong>指根据给定或期望的视点位置</strong>，<strong>生成出对应视角下的二维图像或视频</strong>。常见的视觉合成方法有纹理映射（texture mapping）、着色器（shader）、后处理（post-processing）等。</p>
<p><strong>主要特点</strong></p>
<p><strong>「高质量」：</strong>生成高分辨率、高真实度、高一致性的图像，从而达到与真实世界或传统图形学渲染相媲美甚至超越的效果。</p>
<p><strong>「高效率」：</strong>利用深度学习模型的并行计算和近似推断的能力，大大降低图像合成和渲染的时间和空间复杂度。</p>
<p><strong>「高灵活性」：</strong>根据用户的需求和喜好，对图像进行多样化的操控、变换和编辑，实现个性化和创意化的图像生成。神经渲染的深度生成模型</p>
<p><strong>可能用到的模型</strong></p>
<p><strong>「变分自编码器（VAE）」：</strong>基于概率图模型的生成模型，由编码器和解码器两部分组成，编码器将输入数据映射到一个潜在空间中的随机变量，解码器将潜在变量映射回输出数据。通过最大化输入数据和输出数据之间的条件对数似然，以及最小化潜在变量和先验分布之间的散度，来学习数据的潜在分布和特征。可以用于神经渲染中的语义图像合成与操控，如根据用户给定的语义标签或草图，生成对应的真实图像，并且对图像中的内容进行添加、删除、移动、替换等操作。</p>
<p><strong>「生成对抗网络（GAN）」：</strong>基于博弈论的生成模型，由生成器和判别器两部分组成，生成器将随机噪声或条件输入映射到输出数据，判别器将输入数据判断为真实或伪造。通过最小化生成器和判别器之间的对抗损失，来学习数据的潜在分布和特征。可以用于神经渲染中的目标和场景的新视角合成，如根据用户给定的目标或场景的部分视角，生成其他视角下的图像，并且保持目标或场景的几何结构和光照条件不变。</p>
<p><strong>「自回归模型（AR）」：</strong>基于链式法则的生成模型，它将输出数据分解为一系列条件概率分布，每个分布依赖于之前生成的数据。通过最大化输出数据的联合对数似然，来学习数据的潜在分布和特征，用于神经渲染中的自由视点视频合成，如根据用户给定的视频序列，生成任意视点下的视频，并且保持视频中的动态物体和背景的运动和连贯性不变。</p>
<p><strong>图形学知识</strong></p>
<p><strong>「光线追踪」：</strong>基于物理光学原理的渲染技术，它通过模拟光线从视点出发，在三维空间中与物体表面发生反射、折射、散射等过程，从而计算出每个像素点的颜色和亮度。光线追踪可以用于神经渲染中提供真实感强烈的图像合成和渲染效果，以及提供对深度生成模型训练和推理过程中光照条件变化的约束和指导。</p>
<p><strong>「光照模型」：</strong>基于数学公式的渲染技术，它通过描述光源、物体表面和观察者之间的光照关系，从而计算出每个像素点的颜色和亮度。光照模型可以用于神经渲染中提供不同复杂度和效果的图像合成和渲染效果，以及提供对深度生成模型训练和推理过程中材质和纹理变化的约束和指导。</p>
<p><strong>「几何变换」：</strong>基于线性代数的渲染技术，它通过对三维空间中的物体进行平移、旋转、缩放等操作，从而改变物体的位置、方向和大小。几何变换可以用于神经渲染中提供不同视角和姿态的图像合成和渲染效果，以及提供对深度生成模型训练和推理过程中几何结构变化的约束和指导。</p>
<p><strong>光栅化</strong></p>
<p><strong>可能用到的端到端训练方式</strong></p>
<p><strong>「监督学习」：</strong>基于标注数据的训练方式，它通过给定输入数据和期望输出数据之间的对应关系，来训练深度生成模型。可以用于神经渲染中提供高质量和高精度的图像合成和渲染效果，但是需要大量的标注数据和计算资源。</p>
<p><strong>「无监督学习」：</strong>基于无标注数据的训练方式，它通过利用输入数据或输出数据本身的统计特征或结构信息，来训练深度生成模型。可以用于神经渲染中提供高效率和高灵活性的图像合成和渲染效果，但是需要复杂的模型设计和优化方法。</p>
<p><strong>「弱监督学习」：</strong>介于监督学习和无监督学习之间的训练方式，它通过利用输入数据或输出数据之间的部分或隐含的对应关系，来训练深度生成模型。可以用于神经渲染中提供高质量、高效率和高灵活性的图像合成和渲染效果，但是需要合适的先验知识和约束条件。</p>
<p><strong>神经渲染的应用领域</strong></p>
<ul>
<li><strong>语义图像合成与操控</strong>应用的例子</li>
</ul>
<p><strong>「SPADE」：</strong>基于GAN的语义图像合成方法，它通过使用空间自适应归一化（Spatially-Adaptive Normalization）层，将语义标签图作为生成器的输入，并在每个卷积层中根据语义标签图调整特征图的归一化参数，从而实现了对语义标签图中不同区域内容的精确控制。它能够根据用户给定的任意语义标签图，生成逼真且多样化的真实图像，并且能够对图像中的内容进行添加、删除、移动、替换等操作。</p>
<p><strong>「GauGAN」：</strong>基于SPADE改进的语义图像合成方法，它通过使用自注意力机制（Self-Attention Mechanism）和多尺度判别器（Multi-Scale Discriminator），增强了生成器的感知能力和判别器的区分能力，从而实现了对语义标签图中细节和全局的更好的生成和判断。它能够根据用户给定的任意草图，生成逼真且多样化的真实图像，并且能够对图像中的内容进行添加、删除、移动、替换等操作。</p>
<ul>
<li><strong>目标场景新视角合成</strong>应用的例子</li>
</ul>
<p><strong>「NeRF」：</strong>基于隐函数的新视角合成方法，它通过使用一个深度神经网络，将三维空间中的每个位置映射到一个颜色和不透明度的值，从而隐式地表示一个连续的三维场景。它能够根据用户给定的目标或场景的部分视角，生成其他视角下的图像，并且保持目标或场景的几何结构和光照条件不变。</p>
<p><strong>「NSVF」：</strong>基于体素的新视角合成方法，它通过使用一个稀疏体素网格，将三维空间中的每个体素映射到一个颜色和不透明度的值，从而显式地表示一个离散的三维场景。它能够根据用户给定的目标或场景的部分视角，生成其他视角下的图像，并且保持目标或场景的几何结构和光照条件不变。</p>
<ul>
<li><strong>自由视点视频合成</strong>应用的例子</li>
</ul>
<p><strong>「Neural Volumes」：</strong>基于体素和光场的自由视点视频合成方法，它通过使用一个时变体素网格，将三维空间中每个体素映射到一个颜色和不透明度的值，并且使用一个光场编码器，将每个体素进一步映射到一个光线方向相关的颜色和不透明度的值，从而表示一个动态且具有视差效果的三维场景。Neural Volumes能够根据用户给定的视频序列，生成任意视点下的视频，并且保持视频中的动态物体和背景的运动和连贯性不变。</p>
<p><strong>「Relightables」：</strong>基于神经网络和光场的学习重新打光方法，它通过使用一个神经网络，将三维空间中的每个位置映射到一个颜色和不透明度的值，并且使用一个光场编码器，将每个位置进一步映射到一个光照相关的颜色和不透明度的值，从而表示一个具有光照信息的三维场景。Relightables能够根据用户给定的目标或场景以及期望的光照条件，生成重新打光后的图像，并且保持目标或场景的材质和纹理不变。</p>
<p><strong>「Neural Relighting」：</strong>基于GAN和光照模型的学习重新打光方法，它通过使用一个生成器，将输入图像和期望的光照条件映射到输出图像，并且使用一个判别器，将输出图像和真实图像进行对比。Neural Relighting能够根据用户给定的目标或场景以及期望的光照条件，生成重新打光后的图像，并且保持目标或场景的材质和纹理不变。</p>
<ul>
<li><strong>人体重建渲染</strong>应用的例子</li>
</ul>
<p><strong>「Neural Body」：</strong>基于隐函数和自注意力机制的人体重建渲染方法，它通过使用一个时变隐函数，将三维空间中的每个位置映射到一个颜色和不透明度的值，并且使用一个自注意力机制，将每个位置进一步映射到一个视角相关的颜色和不透明度的值，从而表示一个动态且具有视差效果的人体模型。Neural Body能够根据用户给定的人体图片或视频，生成人体的三维模型，并且能够对人体进行姿态、表情、服装等属性的修改和变换。</p>
<p><strong>「Neural Human」：</strong>基于GAN和几何变换的人体重建渲染方法，它通过使用一个生成器，将输入图片或视频中的人体分割、关键点、姿态等信息映射到输出图片或视频，并且使用一个判别器，将输出图片或视频和真实图片或视频进行对比。Neural Human能够根据用户给定的人体图片或视频，生成人体的三维模型，并且能够对人体进行姿态、表情、服装等属性的修改和变换。</p>
<p><strong>神经渲染面临的挑战</strong></p>
<ul>
<li>技术上面临的挑战：</li>
</ul>
<p><strong>「真实性和一致性」：</strong>神经渲染需要生成与真实世界或传统图形学渲染相媲美甚至超越的图像合成和渲染效果，这需要深度生成模型能够捕捉到数据中的复杂和细微的特征和规律，以及图形学知识能够提供有效和准确的约束和指导。此外，神经渲染还需要保证在不同视角、光照、姿态等条件下，生成的图像具有一致性和连贯性，这需要深度生成模型能够处理数据中的多样性和变化性，以及图形学知识能够提供稳定和可靠的转换和映射。</p>
<p><strong>「复杂性和动态性」：</strong>神经渲染需要处理复杂和动态的场景，如多个物体、多种材质、多个光源、多个运动等，这需要深度生成模型能够表示和生成高维度和高分辨率的数据，以及图形学知识能够模拟和计算复杂的物理过程和效果。此外，神经渲染还需要适应用户的需求和喜好，对图像进行多样化的操控、变换和编辑，这需要深度生成模型能够响应和反馈用户的输入，以及图形学知识能够支持和实现用户的操作。</p>
<p><strong>「开销和资源」：</strong>神经渲染需要消耗大量的数据、计算、内存等资源，这需要深度生成模型能够有效地利用和优化资源的使用，以及图形学知识能够简化和加速资源的处理。此外，神经渲染还需要考虑用户的体验和满意度，对图像进行实时或近实时的合成和渲染，这需要深度生成模型能够快速地训练和推理，以及图形学知识能够并行地渲染和显示。</p>
<ul>
<li>应用上面临的挑战：</li>
</ul>
<p><strong>「质量和可信度」：</strong>神经渲染需要保证生成的图像具有高质量和高可信度，这需要对图像进行有效的评估和保证，如使用客观的指标和标准，如峰值信噪比（PSNR）、结构相似性（SSIM）、感知损失（Perceptual Loss）等，来衡量图像的真实性、一致性、清晰度等；或使用主观的方法和手段，如使用人类评估员或用户反馈，来衡量图像的美观性、满意度、偏好等。</p>
<p><strong>「需求和反馈」：</strong>神经渲染需要满足用户的需求和喜好，这需要对用户进行有效的分析和理解，如使用用户画像（User Profile）、用户行为（User Behavior）、用户情感（User Emotion）等，来获取用户的基本信息、兴趣爱好、情绪状态等；或使用用户交互（User Interaction）、用户反馈（User Feedback）、用户评价（User Evaluation）等，来获取用户的输入输出、意见建议、评分评价等。</p>
<p><strong>「隐私和版权」：</strong>神经渲染需要保护用户的隐私和版权，这需要对数据进行有效的管理和保护，如使用加密（Encryption）、哈希（Hashing）、水印（Watermarking）等，来防止数据被窃取、篡改、泄露等；或使用授权（Authorization）、认证（Authentication）、审计（Audit）等，来防止数据被滥用、侵权、盗用等。</p>
<p>神经渲染是一种将图形学与深度学习相结合的创新方法，它能够实现高质量、高效率、高灵活性的图像合成和渲染，也能够实现多种创意和应用，为图像处理和计算机视觉领域带来了新的可能性和挑战。神经渲染还有着广阔的发展前景和潜力，它可以与其他领域的技术和知识相结合，探索更多的应用场景和领域，促进社会和经济的进步和发展。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-06-01T08:00:00.000Z" title="2024/6/1 16:00:00">2024-06-01</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:39:17.391Z" title="2024/12/4 15:39:17">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E5%AD%A6%E6%9C%AF/">学术</a></span><span class="level-item">6 分钟读完 (大约934个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/06/01/%E5%AD%A6%E6%9C%AF%E4%BA%A4%E6%B5%81%E4%BC%9A%202024.6.1/">医学AI学术交流会</a></p><div class="content"><h3 id="基金项目申请-青年基金评审-赵宏"><a href="#基金项目申请-青年基金评审-赵宏" class="headerlink" title="基金项目申请-青年基金评审-赵宏"></a>基金项目申请-青年基金评审-赵宏</h3><ol>
<li><p>题目很重要</p>
</li>
<li><p>摘要 给出内容和重点</p>
</li>
<li><p>研究背景和意义 </p>
<ol>
<li>研究的紧迫性</li>
<li>研究成果的重要性</li>
</ol>
</li>
<li><p>研究现状 时效性、全面性、针对性、SOTA</p>
</li>
<li><p>研究内容 重点</p>
</li>
<li><p>拟解决的关键问题 来源于研究内容</p>
</li>
<li><p>研究方案和技术路线 </p>
<ol>
<li>与研究内容对应 </li>
<li>体现理论和技术的高度</li>
</ol>
</li>
<li><p>创新点 </p>
<ol>
<li>来源于研究内容，关键问题 </li>
<li>少数即可</li>
</ol>
</li>
<li><p>年度计划、预期成果 </p>
<ol>
<li>与研究内容相对应 </li>
<li>论文、专利、软著、学术交流、人才培养</li>
</ol>
</li>
<li><p>研究基础和条件 </p>
<ol>
<li>已发表的论文、代表作 需要有相关性 </li>
<li>支撑研究的条件</li>
</ol>
</li>
<li><p>其他 师承、工作单位、文笔、领域、地域、科研经历、评审中出现D一票否决</p>
</li>
</ol>
<hr>
<h3 id="医生眼中的人工智能-兰州大学第二医院核磁共振科-胡万均"><a href="#医生眼中的人工智能-兰州大学第二医院核磁共振科-胡万均" class="headerlink" title="医生眼中的人工智能-兰州大学第二医院核磁共振科-胡万均"></a>医生眼中的人工智能-兰州大学第二医院核磁共振科-胡万均</h3><ol>
<li><p>大模型</p>
</li>
<li><p>深度学习神经网络 基于U-net的医学图像问题研究</p>
</li>
<li><p>结合临床实际的研究 前景很好</p>
</li>
<li><p>主要的问题还是 数据集私密的问题 拿不到医学类的数据</p>
</li>
<li><p>医工交叉前景无限 主要是落地到临床实现的应用几乎很少</p>
</li>
<li><p>所有研究都是基于大量数据</p>
</li>
<li><p>缺乏算力  难以进行大模型研究</p>
</li>
<li><p>医学内部的数据量很充分</p>
</li>
<li><p>医学内部的方法学很薄弱 而CS在这方面是强项</p>
</li>
<li><p>医学数据很多 耗费时间很大 而且需要专业的医生或者懂医学的人对模型进行调整 包括对训练结果的监督</p>
</li>
<li><p>利用临床二维图像 基于三维重建模型 生成三维模型 很有意义</p>
</li>
<li><p>医学图像分割</p>
</li>
<li><p>低维图像变高维图像</p>
</li>
<li><p>低分辨率图像变高分辨率图像</p>
</li>
</ol>
<hr>
<h3 id="神经调控研究-脑电-崔琨博"><a href="#神经调控研究-脑电-崔琨博" class="headerlink" title="神经调控研究-脑电-崔琨博"></a>神经调控研究-脑电-崔琨博</h3><ol>
<li>抑郁症<ol>
<li>治疗：经颅磁刺激 TMS</li>
<li>数据采集：脑电采集 设备成本巨大 实验需依靠实验室</li>
</ol>
</li>
<li>源定位<ol>
<li>脑电信号 传导到头皮容易受到其中皮层组织的干扰</li>
<li>脑组织电导率不均匀</li>
<li>脑源信号对各个方向都会施加影响</li>
<li>正问题：从头皮映射到源上 </li>
<li>逆问题：根据头皮脑信号推算出脑内源活动</li>
<li>应用场景：<ol>
<li>边缘计算硬件</li>
<li>对现有框架施加约束项优化定位精度</li>
<li>小型化设备</li>
</ol>
</li>
</ol>
</li>
<li>脑电伪迹去除<ol>
<li>动作影响伪迹</li>
<li>设备伪迹<ol>
<li>脉冲</li>
<li>充放电</li>
</ol>
</li>
<li>滤波去除伪迹</li>
<li>神经网络方法去除眼电</li>
</ol>
</li>
<li>神经调控 <ol>
<li>Neuralink 脑机接口</li>
</ol>
</li>
</ol>
<hr>
<h3 id="基于行为信息的抑郁识别研究-陶博士"><a href="#基于行为信息的抑郁识别研究-陶博士" class="headerlink" title="基于行为信息的抑郁识别研究-陶博士"></a>基于行为信息的抑郁识别研究-陶博士</h3><ol>
<li>通过提取人的面部表情特征点及语音特征点对于抑郁症的识别</li>
</ol>
<hr>
<h3 id="深度学习模型的可解释性-常兆斌"><a href="#深度学习模型的可解释性-常兆斌" class="headerlink" title="深度学习模型的可解释性-常兆斌"></a>深度学习模型的可解释性-常兆斌</h3><ol>
<li>事先可解释性：小样本图像语义分割</li>
<li>主要在语音分割的support与query上进行创新</li>
<li>基于query的信息对support进行指导</li>
<li>提取support与query的信息 找出其共同信息（之间的联系） </li>
<li>主要在特征处理上做 特征解耦 特征重构</li>
</ol>
<hr>
<h3 id="大模型与写作技巧-郭岚"><a href="#大模型与写作技巧-郭岚" class="headerlink" title="大模型与写作技巧-郭岚"></a>大模型与写作技巧-郭岚</h3><ol>
<li>大模型微调</li>
<li>大模型迁移</li>
<li>知识蒸馏</li>
<li>数据优先，只要有其他人没有的数据就能做</li>
<li>写作技巧</li>
<li>生成公式 或 代码优化</li>
</ol>
<hr>
<h3 id="小样本动作识别与因果学习-因果推理的探索-陈志文"><a href="#小样本动作识别与因果学习-因果推理的探索-陈志文" class="headerlink" title="小样本动作识别与因果学习&#x2F;因果推理的探索-陈志文"></a>小样本动作识别与因果学习&#x2F;因果推理的探索-陈志文</h3><ol>
<li>小样本动作识别</li>
<li>因果推理（方法）</li>
</ol>
</div></article></div><div class="card"><div class="card-image"><a class="image is-7by3" href="/2024/03/26/Leetcode/"><img class="fill" src="/img/leetcode/jinx.jpg" alt="Leetcode 算法解析"></a></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2024-03-26T08:00:00.000Z" title="2024/3/26 16:00:00">2024-03-26</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:04:20.908Z" title="2024/12/4 15:04:20">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span> / </span><a class="link-muted" href="/categories/%E7%AE%97%E6%B3%95/Leetcode/">Leetcode</a></span><span class="level-item">29 分钟读完 (大约4423个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/03/26/Leetcode/">Leetcode 算法解析</a></p><div class="content"><hr>
<p><strong>难度：Easy &#x3D; E | Middle &#x3D; M</strong> </p>
<hr>
<p><strong>E 1.两数之和 2024.3.26</strong></p>
<p><strong>方法一：暴力枚举</strong> 两个循环搞定 注意用malloc先分配好空间 int* ret &#x3D; malloc(sizeof(int)*2);</p>
<p>时间O(N^2^) 空间O(1)</p>
<p><strong>！！！</strong>方法二：哈希表 用于方法一的时间复杂度过高——寻找匹配元素</p>
<p>创建一个哈希表，对于每一个x，先查询表中是否存在target-x，再将x插入到表中，保证不会让x与自己匹配</p>
<hr>
<p><strong>E 9.回文数 2024.3.27</strong></p>
<p><strong>方法一：将数字划分为各个单个数进行循环判断；</strong></p>
<p>常规解法：<br>1.先将数字划分为单个数存入数组，while循环加静态数组；<br>2.循环进行判断，注意循环次数；<br>3.可使用动态数组以防止overflow。</p>
<p>方法二：将其转换为字符串数组，直接进行数组元素的相等判断。循环次数到n&#x2F;2即可。</p>
<hr>
<p><strong>E 13.罗马数字转整数 2024.3.29</strong></p>
<p><strong>方法一：字符串的转化 找规律 建立转化表 再进行数字判断并累加</strong></p>
<p>出现了问题“如何建立一个转换表” “switch函数的参数是否能输入多字符结构（字符串）”</p></div><a class="article-more button is-small is-size-7" href="/2024/03/26/Leetcode/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-12-21T09:21:00.000Z" title="2023/12/21 17:21:00">2023-12-21</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:24:56.553Z" title="2024/12/4 15:24:56">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Kunpeng/">Kunpeng</a><span> / </span><a class="link-muted" href="/categories/Kunpeng/Limb/">Limb</a></span><span class="level-item">4 分钟读完 (大约556个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/12/21/limb_compound%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3%E8%AF%95%E5%86%99/">limb_compound开发文档试写</a></p><div class="content"><h2 id="3-3-compound"><a href="#3-3-compound" class="headerlink" title="3.3 compound"></a>3.3 compound</h2><h3 id="3-3-1-接口调用"><a href="#3-3-1-接口调用" class="headerlink" title="3.3.1 接口调用"></a>3.3.1 接口调用</h3><p>第一步：在x86环境调用mathimf.h库中的compound函数</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;stdio.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mathimf.h&quot;</span> <span class="comment">//调用mathimf.h库</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;convert.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">double</span> inputValue1;</span><br><span class="line">    <span class="type">double</span> inputValue2;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;please input an integer1: &quot;</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%llx&quot;</span>, &amp;inputValue1);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;please input an integer2: &quot;</span>);</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">&quot;%llx&quot;</span>, &amp;inputValue2);</span><br><span class="line">    <span class="type">double</span> result = compound(inputValue1,inputValue2); <span class="comment">//调用mathimf.h库中的compound函数</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;The result of _libm_compound is: %llx\n&quot;</span>, *(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span>*)&amp;result);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;The result of _libm_compound is: %lf\n&quot;</span>, *(<span class="type">unsigned</span> <span class="type">long</span> <span class="type">long</span>*)&amp;result);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><a class="article-more button is-small is-size-7" href="/2023/12/21/limb_compound%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3%E8%AF%95%E5%86%99/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-10-30T14:06:00.000Z" title="2023/10/30 22:06:00">2023-10-30</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:22:58.937Z" title="2024/12/4 15:22:58">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Kunpeng/">Kunpeng</a><span> / </span><a class="link-muted" href="/categories/Kunpeng/Limb/">Limb</a></span><span class="level-item">1 分钟读完 (大约215个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/30/%E9%B2%B2%E9%B9%8F%E6%95%B0%E5%AD%A6%E5%BA%93limb%E7%B2%BE%E5%BA%A6%E5%BA%93%20%E9%A1%B9%E7%9B%AE%E6%96%87%E6%A1%A3%E5%8F%8A%E9%83%A8%E5%88%86%E5%91%BD%E4%BB%A4/">鲲鹏数学库limb精度库 项目文档及部分命令</a></p><div class="content"><p><strong>编译命令</strong></p>
<ul>
<li>icc -fp-model precise -no-ftz -g filename.cpp -o filename 生成可执行文件</li>
</ul>
<p><strong>文件</strong></p>
<ul>
<li><p>IDA Pro v7.7版本：<a target="_blank" rel="noopener" href="https://www.52pojie.cn/thread-1640829-1-1.html">https://www.52pojie.cn/thread-1640829-1-1.html</a></p>
</li>
<li><p>类似于IDApro的可以转伪C代码的软件ghidra，<a target="_blank" rel="noopener" href="https://github.com/NationalSecurityAgency/ghidra/releases">https://github.com/NationalSecurityAgency/ghidra/releases</a> 需要java 17</p>
</li>
<li><p>y0f在两个工具上的表现，具有较大差异性，可以参考两者的共性和差异性分析局部代码的实际作用</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/HDK2016/p/10506083.html">https://www.cnblogs.com/HDK2016/p/10506083.html</a><br>IEEE 754浮点的存储定义，是一些基本原理<br>在涉及7F和FF开头的NaN和INF两类极端情况值得参考</p>
</li>
<li><h6 id="Linux下编辑、编译、调试命令总结——gcc和gdb描述"><a href="#Linux下编辑、编译、调试命令总结——gcc和gdb描述" class="headerlink" title="Linux下编辑、编译、调试命令总结——gcc和gdb描述"></a><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yhjoker/p/7533438.html">Linux下编辑、编译、调试命令总结——gcc和gdb描述</a></h6></li>
<li><h6 id="Linux常用（实用）命令大全"><a href="#Linux常用（实用）命令大全" class="headerlink" title="Linux常用（实用）命令大全"></a><a target="_blank" rel="noopener" href="https://www.cnblogs.com/Dengv5/p/16394094.html">Linux常用（实用）命令大全</a></h6></li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/starperfection/article/details/90635654">[C] C语言中的nan和inf使用</a></p>
</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-10-25T08:05:00.000Z" title="2023/10/25 16:05:00">2023-10-25</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:29:42.810Z" title="2024/12/4 15:29:42">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Kunpeng/">Kunpeng</a><span> / </span><a class="link-muted" href="/categories/Kunpeng/Limb/">Limb</a></span><span class="level-item">几秒读完 (大约58个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/25/limb_gdb%E8%B0%83%E8%AF%95/">gdb调试</a></p><div class="content"><p>第一步：gdb .&#x2F;compoundf<br>第二步：打断点 b *0x。。。<br>第三步：layout asm<br>第四步：r<br>第五步：输入<br>第六步：走一步 si<br>第七步：i r 寄存器    看寄存器的值</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-10-13T09:00:00.000Z" title="2023/10/13 17:00:00">2023-10-13</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:25:07.757Z" title="2024/12/4 15:25:07">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Kunpeng/">Kunpeng</a><span> / </span><a class="link-muted" href="/categories/Kunpeng/Limb/">Limb</a></span><span class="level-item">2 分钟读完 (大约244个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/13/limb%E9%A1%B9%E7%9B%AE%E6%97%A5%E5%BF%97/">limb_compound开发项目日志</a></p><div class="content"><h2 id="关于compound-float函数的代码调试及精度测试"><a href="#关于compound-float函数的代码调试及精度测试" class="headerlink" title="关于compound_float函数的代码调试及精度测试"></a>关于compound_float函数的代码调试及精度测试</h2><h3 id="2023-10-13"><a href="#2023-10-13" class="headerlink" title="2023.10.13"></a>2023.10.13</h3><p>当前状态：代码未通过编译</p>
<ol>
<li><p>‘<strong>_libm_error_support</strong>‘ 报错为 未声明的函数 <strong>此函数是否为库函数 是否需要进行链接</strong> </p>
<p>解决办法：直接注释</p>
</li>
<li><p>‘<strong>_libm_exp</strong>‘ 同上</p>
</li>
<li><p>compound_float0.c:198:1: <strong>warning</strong>: non-void function does not return a value in all control paths [-Wreturn-type]</p>
<p>非void函数 在每个分支都没有返回值 ida中反汇编的源代码函数为void类型</p>
</li>
<li><p><strong>log1p</strong> 在ida源代码中为**_libm_log1p** vscode中报错为未定义的函数</p>
</li>
</ol></div><a class="article-more button is-small is-size-7" href="/2023/10/13/limb%E9%A1%B9%E7%9B%AE%E6%97%A5%E5%BF%97/#more">阅读更多</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2023-09-28T09:03:00.000Z" title="2023/9/28 17:03:00">2023-09-28</time>发表</span><span class="level-item"><time dateTime="2024-12-04T07:25:27.288Z" title="2024/12/4 15:25:27">2024-12-04</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/Kunpeng/">Kunpeng</a><span> / </span><a class="link-muted" href="/categories/Kunpeng/Limb/">Limb</a></span><span class="level-item">5 分钟读完 (大约743个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/09/28/limb%E8%A7%84%E8%8C%83%E6%A0%87%E5%87%86%EF%BC%88%E8%AF%95%E8%A1%8C%EF%BC%89/">limb规范标准（试行）</a></p><div class="content"><p><strong>未规范代码</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">__int64 sincos_k32(double *a1, double *a2,int a3, double a4)&#123;</span><br><span class="line">    double v11_data[2];</span><br><span class="line">    unsigned __int64 v4;</span><br><span class="line">    __int64 result;</span><br><span class="line">    unsigned int v5;</span><br><span class="line">    int v6;</span><br><span class="line">    double v7;</span><br><span class="line">    unsigned int v8;</span><br><span class="line">    int v9;</span><br><span class="line">    struct __m128d v10;</span><br><span class="line">    struct __m128d v11;</span><br><span class="line">    int v12;</span><br><span class="line">    int v13;</span><br><span class="line">    int v14;</span><br><span class="line">    double v15;</span><br><span class="line">    double v16;</span><br><span class="line">    __int64 v17;</span><br><span class="line">    double v18;</span><br><span class="line">    double v19;</span><br><span class="line">    double v21;</span><br><span class="line">    double v22;</span><br><span class="line">    double inputX;</span><br><span class="line">    double v24;</span><br><span class="line">    double v25;</span><br><span class="line">    double v26;</span><br><span class="line">    double v27;</span><br><span class="line">    double v28;</span><br><span class="line"></span><br><span class="line">    double ones[2] = &#123;1.0, -1.0&#125;;</span><br><span class="line">    double iones[2]=&#123;1,-1&#125;;</span><br><span class="line"></span><br><span class="line">    v28 = a4;</span><br><span class="line"></span><br><span class="line">    v4 = HIDWORD(a4) &gt;&gt; 31;</span><br><span class="line"></span><br><span class="line">    v5 = HIDWORD(a4) &amp; 0x7FFFFFFF;//将上述读取的 64 位整数值与 0x7FFFFFFF 进行按位与操作。</span><br><span class="line"></span><br><span class="line">    v6 = iones[v4] * a3;</span><br><span class="line"></span><br><span class="line">    v7 = fabs(a4);</span><br><span class="line"></span><br><span class="line">    if (v5 &gt;= 0x41C00000) &#123;</span><br><span class="line">        v28 = v7;</span><br><span class="line">        v17 = reduce_pi04d(&amp;v28, v6, v7);</span><br><span class="line">        v8 = v17 + 1;</span><br><span class="line">        v7 = v28;</span><br><span class="line">	   	//v9 = ((_BYTE)(v17) + 1) &amp; 2;// 是从一个 double 类型的值中提取指定索引位置的字节，并对提取的字节进行处理，最终得到一个 uint8_t 类型的结果。</span><br><span class="line">		//v9 = (getByteFromDouble(v17, 0) + 1) &amp; 2;</span><br><span class="line">        v9 = ((_BYTE)v17 + 1) &amp; 2;</span><br><span class="line">    &#125; else if (v5 &gt; 0x3FF90000) &#123;</span><br><span class="line">        v10.m128d_f64[0] = Asdouble(0x3FF45F306DC9C883) * v7;</span><br><span class="line">        v28 = v7;</span><br><span class="line">        v27 = Asdouble(0x3FF45F306DC9C883) * v7 + Asdouble(0x4338000000000000);</span><br><span class="line">        //v11_data[1] = (double)SLODWORD(v27);</span><br><span class="line">		//v11 = *((__m128d*)v11_data);</span><br><span class="line">		v11.m128d_f64[0] = (double)SLODWORD(v27);</span><br><span class="line">		//v11.m128d_f64[1] = v11_data[1];</span><br><span class="line">        v12 = compare(&amp;v10, &amp;v11);</span><br><span class="line">        v13 = LODWORD(v27) - v12 + v6;</span><br><span class="line">        v14 = v13 &amp; 1;</span><br><span class="line">        v8 = v14 + v13;</span><br><span class="line">        v15 = (double)(v14 + LODWORD(v27) - v12);</span><br><span class="line">        v9 = v8 &amp; 2;</span><br><span class="line">        if (v5 &gt;= 0x41000000)&#123;</span><br><span class="line">            v16 = v28 - Asdouble(0x3FE921FB40000000) * v15 - Asdouble(0x3E64442D00000000) * v15 - Asdouble(0x3CE8469880000000) * v15;</span><br><span class="line">        &#125;</span><br><span class="line">        else&#123;</span><br><span class="line">            v16 = v7 - Asdouble(0x3FE921FB54440000) * v15 - Asdouble(0x3D768C234C400000) * v15;</span><br><span class="line">        &#125;</span><br><span class="line">        v7 = v16 - v15 * Asdouble(0x3B68CC51701B839A);</span><br><span class="line">        v28 = v7;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    else&#123;</span><br><span class="line">        v8 = v6 + 1;</span><br><span class="line">        v9 = v8 &amp; 2;</span><br><span class="line">        if (v8 &amp; 2)&#123;</span><br><span class="line">            v7 = v7 - Asdouble(0x3FE921FB54442D18);</span><br><span class="line">        &#125;</span><br><span class="line">        v28 = v7;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    v18 = v7 * v7;</span><br><span class="line"></span><br><span class="line">    v19 = (Asdouble(0x3EC71D9AA585BFC4) * (v18 * v18) </span><br><span class="line">        + Asdouble(0x3F81111110FD4208)) * (v18 * v18) * v7</span><br><span class="line">        + v7</span><br><span class="line">        + ((Asdouble(0xBE5AA2880297FC43) * (v18 * v18) </span><br><span class="line">        + Asdouble(0xBF2A019FD9BD0882)) * (v18 * v18) </span><br><span class="line">        + Asdouble(0xBFC555555555516D))* v18* v7;</span><br><span class="line">    result = ((v8 + 2) &gt;&gt; 2) &amp; 1;</span><br><span class="line"></span><br><span class="line">    v21 = (Asdouble(0x3EFA01299942AB00) * (v18 * v18) </span><br><span class="line">        + Asdouble(0x3FA5555555150951)) * (v18 * v18)</span><br><span class="line">        + Asdouble(0x3FF0000000000000)</span><br><span class="line">        + ((Asdouble(0x0BE9247507B5EE59E) * (v18 * v18) </span><br><span class="line">        + Asdouble(0xBF56C16BAE710FF8)) * (v18 * v18) </span><br><span class="line">        + Asdouble(0xBFDFFFFFFFFFE6A2)) * v18;</span><br><span class="line"></span><br><span class="line">    v22 = ones[(v8 &gt;&gt; 2) &amp; 1 ^ v4];</span><br><span class="line"></span><br><span class="line">    if (v9)&#123;</span><br><span class="line">        inputX = v21 * v22;</span><br><span class="line">        v24 = ones[result];</span><br><span class="line">        *a1 = inputX;</span><br><span class="line">        *a2 = v19 * v24;</span><br><span class="line">    &#125;</span><br><span class="line">    else&#123;</span><br><span class="line">        v25 = v19 * v22;</span><br><span class="line">        v26 = ones[result];</span><br><span class="line">        *a1 = v25;</span><br><span class="line">        *a2 = v21 * v26;</span><br><span class="line">    &#125;</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><a class="article-more button is-small is-size-7" href="/2023/09/28/limb%E8%A7%84%E8%8C%83%E6%A0%87%E5%87%86%EF%BC%88%E8%AF%95%E8%A1%8C%EF%BC%89/#more">阅读更多</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/4/">上一页</a></div><div class="pagination-next"><a href="/page/6/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/4/">4</a></li><li><a class="pagination-link is-current" href="/page/5/">5</a></li><li><a class="pagination-link" href="/page/6/">6</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/head_circle.png" alt="BONESKEEP"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">BONESKEEP</p><p class="is-size-6 is-block">硕士研究生</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Lanzhou University of Technology</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">51</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">6</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/BONESKEEP" target="_blank" rel="me noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/BONESKEEP"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="QQ" href="https://qm.qq.com/q/4kKCYBUmaA"><i class="fab fab fa-qq"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Mail" href="mailto:coolhui2020@163.com"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/BONESKEEP" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="https://boneskeep.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Blog</span></span><span class="level-right"><span class="level-item tag">boneskeep.github.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Game/"><span class="level-start"><span class="level-item">Game</span></span><span class="level-end"><span class="level-item tag">2</span></span></a><ul><li><a class="level is-mobile" href="/categories/Game/Games101/"><span class="level-start"><span class="level-item">Games101</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Kunpeng/"><span class="level-start"><span class="level-item">Kunpeng</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/Kunpeng/Limb/"><span class="level-start"><span class="level-item">Limb</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">28</span></span></a><ul><li><a class="level is-mobile" href="/categories/Linux/App/"><span class="level-start"><span class="level-item">App</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/"><span class="level-start"><span class="level-item">学术</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/3DGS/"><span class="level-start"><span class="level-item">3DGS</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/NeRF/"><span class="level-start"><span class="level-item">NeRF</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA/"><span class="level-start"><span class="level-item">三维重建</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%AD%A6%E6%9C%AF/%E5%91%BD%E4%BB%A4/"><span class="level-start"><span class="level-item">命令</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">8</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/Leetcode/"><span class="level-start"><span class="level-item">Leetcode</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li></ul></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><figure class="media-left"><a class="image" href="/2024/12/23/2024-12-23-Leetcode-Hot100-No.198-%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/"><img src="/img/leetcode/Shangri-La%20Frontier-01.jpg" alt="Leetcode Hot100 No.198 打家劫舍"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-12-23T13:32:00.000Z">2024-12-23</time></p><p class="title"><a href="/2024/12/23/2024-12-23-Leetcode-Hot100-No.198-%E6%89%93%E5%AE%B6%E5%8A%AB%E8%88%8D/">Leetcode Hot100 No.198 打家劫舍</a></p><p class="categories"><a href="/categories/%E7%AE%97%E6%B3%95/">算法</a> / <a href="/categories/%E7%AE%97%E6%B3%95/Leetcode/">Leetcode</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/12/14/2024-12-14-Leetcode-Hot100-No.56-%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/"><img src="/img/leetcode/Shangri-La%20Frontier-01.jpg" alt="Leetcode Hot100 No.56 合并区间"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-12-13T16:18:00.000Z">2024-12-14</time></p><p class="title"><a href="/2024/12/14/2024-12-14-Leetcode-Hot100-No.56-%E5%90%88%E5%B9%B6%E5%8C%BA%E9%97%B4/">Leetcode Hot100 No.56 合并区间</a></p><p class="categories"><a href="/categories/%E7%AE%97%E6%B3%95/">算法</a> / <a href="/categories/%E7%AE%97%E6%B3%95/Leetcode/">Leetcode</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/12/08/2024-12-08-Leetcode-Hot100-No.73-%E7%9F%A9%E9%98%B5%E7%BD%AE%E9%9B%B6/"><img src="/img/leetcode/Shangri-La%20Frontier-01.jpg" alt="Leetcode Hot100 No.73 矩阵置零"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-12-08T15:10:00.000Z">2024-12-08</time></p><p class="title"><a href="/2024/12/08/2024-12-08-Leetcode-Hot100-No.73-%E7%9F%A9%E9%98%B5%E7%BD%AE%E9%9B%B6/">Leetcode Hot100 No.73 矩阵置零</a></p><p class="categories"><a href="/categories/%E7%AE%97%E6%B3%95/">算法</a> / <a href="/categories/%E7%AE%97%E6%B3%95/Leetcode/">Leetcode</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/12/07/2024-12-07-Leetcode-Hot100-No.53-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%95%B0%E7%BB%84%E5%92%8C/"><img src="/img/leetcode/Shangri-La%20Frontier-01.jpg" alt="Leetcode Hot100 No.53 最大子数组和"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-12-07T12:11:00.000Z">2024-12-07</time></p><p class="title"><a href="/2024/12/07/2024-12-07-Leetcode-Hot100-No.53-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%95%B0%E7%BB%84%E5%92%8C/">Leetcode Hot100 No.53 最大子数组和</a></p><p class="categories"><a href="/categories/%E7%AE%97%E6%B3%95/">算法</a> / <a href="/categories/%E7%AE%97%E6%B3%95/Leetcode/">Leetcode</a></p></div></article><article class="media"><figure class="media-left"><a class="image" href="/2024/12/06/2024-12-06-Leetcode-Hot100-No.15-%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/"><img src="/img/leetcode/Shangri-La%20Frontier-01.jpg" alt="Leetcode Hot100 No.15 三数之和"></a></figure><div class="media-content"><p class="date"><time dateTime="2024-12-06T09:26:00.000Z">2024-12-06</time></p><p class="title"><a href="/2024/12/06/2024-12-06-Leetcode-Hot100-No.15-%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8C/">Leetcode Hot100 No.15 三数之和</a></p><p class="categories"><a href="/categories/%E7%AE%97%E6%B3%95/">算法</a> / <a href="/categories/%E7%AE%97%E6%B3%95/Leetcode/">Leetcode</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/12/"><span class="level-start"><span class="level-item">十二月 2024</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">十月 2024</span></span><span class="level-end"><span class="level-item tag">29</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/09/"><span class="level-start"><span class="level-item">九月 2024</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/06/"><span class="level-start"><span class="level-item">六月 2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/03/"><span class="level-start"><span class="level-item">三月 2024</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/12/"><span class="level-start"><span class="level-item">十二月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/10/"><span class="level-start"><span class="level-item">十月 2023</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/09/"><span class="level-start"><span class="level-item">九月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/04/"><span class="level-start"><span class="level-item">四月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Games101/"><span class="tag">Games101</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kunpeng/"><span class="tag">Kunpeng</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Leetcode/"><span class="tag">Leetcode</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">28</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E6%9C%AF/"><span class="tag">学术</span><span class="tag">7</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/head_circle.png" alt="BONESKEEP&#039; BLOG" height="28"></a><p class="is-size-7"><span>&copy; 2024 BONESKEEP</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">Open Source</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>